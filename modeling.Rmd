---
title: "NEID Solar Data -- Linear Modeling"
author: "Joe Salzer"
date: "`r Sys.Date()`"
output: html_document
---

This file is rstudio script for testing all of our modeling

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
source("readSpectra.R")
library(Matrix)
library(parallel)
library(pbmcapply)
```

# linear modeling

these will be passed to terminal
```{r}
covariateNames = c("fit_gauss_a", "fit_gauss_b", "fit_gauss_depth", "fit_gauss_sigmasq",str_c("proj_hg_coeff_", c(0,seq(2,10,1))))
# covariateNames = c("fit_gauss_a", "fit_gauss_b", "fit_gauss_depth", "fit_gauss_sigmasq",str_c("proj_thg_coeff_", c(0,seq(2,10,1))))
#covariateNames = c()

print(covariateNames)
```

Rscript rv_lm.R completeLines.csv

fit_gauss_a,fit_gauss_b,fit_gauss_depth,fit_gauss_sigmasq,proj_hg_coeff_0,proj_hg_coeff_2,proj_hg_coeff_3,proj_hg_coeff_4,proj_hg_coeff_5,proj_hg_coeff_6,proj_hg_coeff_7,proj_hg_coeff_8,proj_hg_coeff_9,proj_hg_coeff_10

```{r}
# get csv file name
csvFileName = "completeLines.csv"
```

## setting variables and loading data

variables to be set by the user

```{r}
# working directory with the data
WD_DATA = "/Users/josephsalzer/research/exostat/"
# RESPONSE variable
RESPONSE = "rv_template_0.5"
# names of the timeID, lineID, and timeGroupID
TIME_ID_NAME = "date"
LINE_ID_NAME = "line_order"
TIMEGROUP_ID_NAME = "date_groups"
# csv file name
completeLines_df = read_csv(str_c(WD_DATA,csvFileName)) %>%
  mutate(!!TIME_ID_NAME := as.Date( !!sym(TIME_ID_NAME) ))
```


## get list of lineIDs and timeIDs

```{r}
# vec of LineIDs  in completeLines_df
lineIDs = completeLines_df %>% group_by(!!sym(LINE_ID_NAME)) %>% summarize(n. = n()) %>% pull(!!sym(LINE_ID_NAME))
# vec of timeIDs in completeLines_df
timeIDs = completeLines_df %>% group_by(!!sym(TIME_ID_NAME)) %>% summarize(n. = n()) %>% pull(!!sym(TIME_ID_NAME))
T_ = length(timeIDs)
L_ = length(lineIDs)
```

## create model formula and subdirectories

the following chunk is the typical specification of the linear models as described in the paper and included in the script "rv_lm.R"

```{r}
# create directory called "models" in working directory
if (!dir.exists(str_c(WD_DATA,"models"))) {dir.create(str_c(WD_DATA,"models"))}

# base TWFE formula
twfe_formula = paste0("~ 0 ")

# if covariateNames aren't empty, include interactions, else use TWFE
if (!is_empty(covariateNames)) {
  
  # build interactions with LINE_ID_NAME for each provided centered covariate
  covar_slopes = paste0(LINE_ID_NAME,":", covariateNames, "_centered", collapse = " + ")
  # construct the full formula
  modelFormula = as.formula(paste(twfe_formula, covar_slopes, sep = " + "))
  
  # name the current model, using Gaussian fit parameters and hg covariateNames
  ## remove or edit below lines if using other covariates ##
  gaussCovars = covariateNames[startsWith(covariateNames,"fit_gauss")]
  gaussCovars_names = paste(str_split_i(gaussCovars, "_", i = 3),collapse=",")
  if (gaussCovars_names=="") {gaussCovars_names = "none"} else if(length(gaussCovars) == 4) {gaussCovars_names = "all"}
  hgCovars = covariateNames[startsWith(covariateNames,"proj_hg_coeff_")]
  hgCovars_names = paste(str_split_i(hgCovars, "_", i = 4),collapse=",")
  if (hgCovars_names=="") {hgCovars_names = "none"} else if(length(hgCovars) == 10) {hgCovars_names = "all"}
  model_name = str_c("Gauss=",gaussCovars_names,"_HG=",hgCovars_names)
  ## remove or edit above lines if using other covariates ##
  
  # create model directory
  model_dir = str_c(WD_DATA,"models/",model_name)
  if (!dir.exists(model_dir)) {dir.create(model_dir)}

  rm(twfe_formula,covar_slopes,gaussCovars,gaussCovars_names,hgCovars,hgCovars_names)
} else {
  # if empty, just fit TWFE
  modelFormula = as.formula(twfe_formula)
  # model name
  model_name = "TWFE"
  # create model directory
  model_dir = str_c(WD_DATA,"models/",model_name)
  if (!dir.exists(model_dir)) {dir.create(model_dir)}
  rm(twfe_formula)
}

cat("The model formula for the covariates is:\n", as.character(modelFormula), "\n")
cat("The model name is:", model_name, "\n")
```

below is the specification for the Common Slopes Model for covariateNames

*COMMON SLOPES*
```{r}
modelFormula = as.formula(paste("~ 0 ", paste0(covariateNames, "_centered", collapse = " + "), sep = " + "))
# model name
model_name = "CommonSlopes"
# create model directory
model_dir = str_c(WD_DATA,"models/",model_name)
if (!dir.exists(model_dir)) {dir.create(model_dir)}

cat("The model formula for the covariates is:\n", as.character(modelFormula), "\n")
cat("The model name is:", model_name, "\n")
```

## standardize dataframe

center by lineID, scaling each continuous column. We can "inject" a planet here, by changing the amplitude, frequency, and horizontal offset 

*no planet*
```{r}
# get standardized rv arranging by line_order and then date
standardized_list = completeLines_df %>%
  standardize(covariates = covariateNames)

rv_df = standardized_list$train.df %>%
  arrange(LINE_ID_NAME, TIME_ID_NAME) %>%
  mutate(timeID = factor(!!sym(TIME_ID_NAME)),
         lineID = factor(!!sym(LINE_ID_NAME)))

# set responses
responses = rv_df[[RESPONSE]]
```

*with planet*
```{r}
AMP = 3
HORIZONTAL_OFFSET = 0
VERTICAL_OFFSET = 0
FREQ = 2*pi/250

# get standardized rv, with an injected planet, arranging by line_order and then date
rv_df = ( injectPlanet(completeLines_df, amp = AMP, freq = FREQ, horizontal_offset = HORIZONTAL_OFFSET, vertical_offset = VERTICAL_OFFSET) %>%
  standardize(covariates = covariateNames ) )$train.df %>%
  arrange(!!sym(LINE_ID_NAME), !!sym(TIME_ID_NAME))

# mean offset for non-noisy planet signal, noisy planet signal, and noise (original rv signal)
rv_df %>%
  select(date,date_groups,pert_val,pert_rv,rv_template_0.5) %>%
  unique() %>%
  group_by(date_groups) %>%
  summarize(planet_offset = mean(pert_val),
            noisePlanet_offset = mean(pert_rv),
            noise_offset = mean(rv_template_0.5))

estimated_rv = rv_df %>%
  group_by(date) %>%
  summarize(est_rv = mean(pert_rv))

seq_days = seq(min(estimated_rv$date), max(estimated_rv$date), by = 1)

ggplot() +
  geom_point(mapping = aes(x = estimated_rv$date, y = estimated_rv$est_rv)) +
  geom_line(mapping = aes(x = seq_days,
                          y = VERTICAL_OFFSET + AMP*sin(FREQ*changeFun(as.numeric(seq_days))+HORIZONTAL_OFFSET) ) )

# set responses
responses = rv_df$pert_rv

rm(AMP,HORIZONTAL_OFFSET,VERTICAL_OFFSET,FREQ,estimated_rv,seq_days)
```


## set contrasts and make design matrix

*group sum2zero for timeIDs, sum2zero for lineIDs (single intercept, single slope model)*

```{r}
# get group sizes
group_sizes = rv_df %>%
  group_by(!!sym(TIMEGROUP_ID_NAME)) %>%
  summarize(size = n()/L_) %>%
  pull(size)
```

```{r}
# time fixed effects encoding matrix
S_time = cbind(
  bdiag(lapply(group_sizes, function(n) rep(1, n))) %*% contr.sum(length(group_sizes)),
  bdiag(lapply(group_sizes, function(n) contr.sum(n) ))
  )
# time fixed effects design matrix
X_time = kronecker(rep(1,L_), S_time)
colnames(X_time) = str_c("alpha",1:ncol(X_time))
dim(X_time)
```

```{r}
# design matrix for the line fixed effects
X_line = kronecker( contr.sum(L_, sparse = T), rep(1,T_) )
colnames(X_line) = str_c("beta",1:ncol(X_line))
dim(X_line)
```

```{r}
# design matrix for the covariates
X_covar = sparse.model.matrix(modelFormula,rv_df) 
dim(X_covar)
```

```{r}
# create full design matrix
designMat = cbind(rep(1,L_*T_),X_time,X_line,X_covar)
dim(designMat)

rm(X_time,X_line,X_covar)
```

```{r}
T_+L_+L_*14-1
T_+L_+L_*0-1
```

*group sum2zero for timeIDs, sum2zero for lineIDs (multiple intercept, single slope model)*

```{r}
# get group sizes
group_sizes = rv_df %>%
  group_by(!!sym(TIMEGROUP_ID_NAME)) %>%
  summarize(size = n()/L_) %>%
  pull(size)
```

```{r}
# time fixed effects encoding matrix
S_time = bdiag(lapply(group_sizes, function(n) contr.sum(n) ))
# time fixed effects design matrix
X_time = kronecker(rep(1,L_), S_time)
dim(X_time)
```

```{r}
# design matrix for the line-group fixed effects
X_line = kronecker(diag(L_), bdiag(lapply(group_sizes, function(n) rep(1, n))))
dim(X_line)
```
```{r}
# design matrix for the covariates
X_covar = sparse.model.matrix(modelFormula,rv_df) 
dim(X_covar)
```

```{r}
# create full design matrix
designMat = cbind(X_time,X_line,X_covar)
dim(designMat)

rm(X_time,X_line,X_covar)
```

```{r}
T_+2*L_+L_*14-2
T_+2*L_+L_*0-2
```

*NOTATION FOR THE MISS model, line_group fixed effects*

```{r}
L_ = 3
g = c(3, 2)  # Can be any vector

# number of groups
G_ = length(g)

# Sum-to-zero contrast matrix
S = contr.sum(L_ * G_)

# Construct block diagonal matrix dynamically
R = kronecker(diag(L_), bdiag(lapply(g, function(n) rep(1, n))))

R
# Compute X
X = R %*% S
X
```


## fit model

```{r}
# fit the linear model using all data
fit_lm = sparseLM(designMat, responses)
```

## get cleaned RVs

```{r}
# initialize 0's in the linear operator
linear_op_mat = Matrix(0, nrow = T_, ncol = length(fit_lm$beta_hat[,1]), sparse = T )
# matrix for estimating the cleaned RV
linear_op_mat[,(length(group_sizes)+1):sum(group_sizes)] = bdiag(lapply(group_sizes, function(n) contr.sum(n) ))
```

```{r}
# covariance matrix of model parameters
cov_mat = fit_lm$var_beta_hat

# dataframe of date's cleaned rv
cleanRV_df = data.frame(
  timeID = as.Date(timeIDs)
)
# find the mle estiamte, var, and se for the intercept plus alpha
cleanRV_df$estimate = (linear_op_mat %*% fit_lm$beta_hat[,1] )[,1]
cleanRV_covar = linear_op_mat %*% cov_mat %*% t(linear_op_mat)
cleanRV_df$var = diag( cleanRV_covar )
cleanRV_df$se = sqrt(cleanRV_df$var)
# find rmse of the clean RV
RMSE = rmse_t(0,cleanRV_df$estimate)
```


## save fit

```{r}
saveRDS(list(designMat = designMat,
             responses = responses,
             df = rv_df,
             group_sizes = group_sizes,
             modelFormula = modelFormula,
             covariateNames = covariateNames,
             fit_lm = fit_lm,
             cleanRV_df = cleanRV_df,
             RMSE = RMSE),
        file = str_c(model_dir, "/model.rds" ) )
```

# leverage calculations

```{r}
# arguments passed to command line
# model name
MODEL_NAME = "Gauss=all_HG=all"
BATCH_SIZES = 1000
```

```{r}
# working directory with the data
WD_DATA = "/Users/josephsalzer/research/exostat/"
# read in the model fit
model_fit = readRDS(str_c(WD_DATA, "models/", MODEL_NAME, "/model.rds"))
fit_lm = model_fit$fit_lm
X = model_fit$designMat
XtX_inv = (fit_lm$var_beta_hat)/fit_lm$sigma2_hat
```

```{r}
# directory containing the leverage files
leverage_dir = str_c(WD_DATA, "models/", MODEL_NAME, "/leverages")
# create directory called "leverages" in model name
if ( !dir.exists(leverage_dir) ) {
  dir.create(leverage_dir)
}
```

```{r}
# optimized leverage computation
compute_leverage_batches = function(indices) {
  start_row = indices[1]
  end_row = indices[2]
  
  # extract rows in a batch
  rows = X[start_row:end_row, , drop = FALSE]
  # compute leverage for all rows in the batch
  leverage = rowSums(rows * (rows %*% XtX_inv))
  
  names(leverage) = str_c("row",start_row:end_row)

  # save results in a single file for the batch
  batch_filename = file.path(leverage_dir, str_c("leverage_", start_row, "_", end_row, ".rds"))
  saveRDS(leverage, file = batch_filename)
}

# function to create batches
create_batches = function(n_rows, batch_size) {
  # start indices of batches
  starts = seq(1, n_rows, by = batch_size)
  # end indices of batches
  ends = pmin(starts + batch_size - 1, n_rows)
  # combine into a list of start and end indices
  batches = Map(c, starts, ends)
  return(batches)
}
```

```{r}
# create batch indices
batches = create_batches(nrow(X), BATCH_SIZES)
```

```{r}
# test
compute_leverage_batches(batches[[1]])
```

```{r}
# parallel computation of leverage
result <- pbmclapply(batches, FUN = compute_leverage_batches, mc.cores = 7)
```

```{r}
# get the list of all .rds files in the leverage directory
leverage_files = list.files(path = leverage_dir, pattern = "\\.rds$", full.names = TRUE)
# initialize an empty vector to store the combined leverage values
leverages = vector()
for (file in leverage_files) {
  leverages = c(leverages, readRDS(file))
}
rm(leverage_files)
```

```{r}
head(leverages)
length(leverages)
```

```{r}
saveRDS(leverages,
        str_c(WD_DATA, "models/", MODEL_NAME, "/leverages.rds" ))
```

# bootstrap coefficients and residuals

```{r}
# arguments passed to command line
# model name
MODEL_NAME = "Gauss=all_HG=all"
NUM_BOOTS = 2500
```

```{r}
# working directory with the data
WD_DATA = "/Users/josephsalzer/research/exostat/"
# names of the timeID, lineID, and timeGroupID
TIME_ID_NAME = "date"
LINE_ID_NAME = "line_order"
```

```{r}
# directory containing the bootstrap files
bootstrap_dir = str_c(WD_DATA, "models/", MODEL_NAME, "/wild_bootstraps" )
# create directory called "bootstrap" in model name
if ( !dir.exists(bootstrap_dir) ) {
  dir.create(bootstrap_dir)
}
```

```{r}
# read in the model fit
model_fit = readRDS(str_c(WD_DATA, "models/", MODEL_NAME, "/model.rds"))
# lm fit, df, design matrix and leverages
fit_lm = model_fit$fit_lm
rv_df = model_fit$df
designMat = model_fit$designMat
leverages = readRDS(str_c(WD_DATA, "models/", MODEL_NAME, "/leverages.rds" ))

# vec of LineIDs  in completeLines_df
lineIDs = rv_df %>% group_by(!!sym(LINE_ID_NAME)) %>% summarize(n. = n()) %>% pull(!!sym(LINE_ID_NAME))
# vec of timeIDs in completeLines_df
timeIDs = as.Date(rv_df %>% group_by(!!sym(TIME_ID_NAME)) %>% summarize(n. = n()) %>% pull(!!sym(TIME_ID_NAME)))
  
T_ = length(timeIDs)
L_ = length(lineIDs)
```


```{r}
# get modified residuals
u_hat = fit_lm$resid/(1-leverages)
```

assign blocks for the bootstrap
```{r}
# assign blocks for the bootstrap (by line)
block_size = length(timeIDs)

rv_df = rv_df %>%
  arrange(!!sym(LINE_ID_NAME),!!sym(TIME_ID_NAME)) %>%
  mutate(boot_block = factor( ceiling(row_number() / block_size) ) )
```

```{r}
wildBoot_parallel = function(seedID) {
  
  # set a given seed for replication
  set.seed(seedID)
  
  # ensure that the rv_df is arranged first by lineID and then timeID, and that these are factors
  rv_df = rv_df %>%
    arrange(!!sym(LINE_ID_NAME), !!sym(TIME_ID_NAME)) %>%
    mutate( lineID = factor(!!sym(LINE_ID_NAME)),
            timeID = factor(!!sym(TIME_ID_NAME)))
  
  # rademacher random variable for each block in the dataset
  v = sample( c(-1, 1), size = length(lineIDs), replace = TRUE)
  # generate wild bootstrap residuals by block
  u_star = u_hat*v[rv_df$boot_block]
  
  # construct bootstrap sample
  y_star = fit_lm$y_hat + u_star
  # re-estimate the model on bootstrap sample
  fitstar_lm = sparseLM(designMat, y_star, PRINT_TIME = F)

  # store data
  saveRDS(fitstar_lm$beta_hat[,1],
          file = str_c(bootstrap_dir, "/bootstraps_list_", seedID, ".rds" ) )
  
}
```

```{r}
wildBoot_parallel(1)
```

run wild bootstrap

```{r}
boot_straps <- pbmclapply(100*( c(1:NUM_BOOTS) + 2000 ), FUN = wildBoot_parallel, mc.cores = 7)
rm(boot_straps)
```

# cross validation

```{r}
# pass on command line

# model name
MODEL_NAME = "Gauss=all_HG=all"
# number of timeIDs in each leave-one-out cv
CV_NUM_DAY = 0
```

```{r}
# working directory with the data
WD_DATA = "/Users/josephsalzer/research/exostat/"
# RESPONSE variable
RESPONSE = "rv_template_0.5"
# names of the timeID, lineID, and timeGroupID
TIME_ID_NAME = "date"
LINE_ID_NAME = "line_order"
TIMEGROUP_ID_NAME = "date_groups"
# read in the model fit
model_fit = readRDS(str_c(WD_DATA, "models/", MODEL_NAME, "/model.rds" ))
df = model_fit$df
covariateNames = model_fit$covariateNames
modelFormula = model_fit$modelFormula
```

```{r}
# vec of LineIDs  in completeLines_df
lineIDs = df %>% group_by(!!sym(LINE_ID_NAME)) %>% summarize(n. = n()) %>% pull(!!sym(LINE_ID_NAME))
# vec of timeIDs in completeLines_df
timeIDs = as.Date(df %>% group_by(!!sym(TIME_ID_NAME)) %>% summarize(n. = n()) %>% pull(!!sym(TIME_ID_NAME)))
  
T_ = length(timeIDs)
L_ = length(lineIDs)
```

```{r}
# create directory called "cv" in working directory
if ( !dir.exists(str_c(WD_DATA,"models/",MODEL_NAME,"/cv_block=",2*CV_NUM_DAY)) ) {
  dir.create(str_c(WD_DATA,"models/",MODEL_NAME,"/cv_block=",2*CV_NUM_DAY))
}
```

```{r}
# list of timeIDs to be left out
left_out_timeIDs = create_sliding_windows(timeIDs, CV_NUM_DAY)
left_out_timeIDs[[1]]
left_out_timeIDs[[2]]
left_out_timeIDs[[15]]
left_out_timeIDs[[227]]
left_out_timeIDs[[228]]

# summary of the size of each left-out set
summary( sapply(left_out_timeIDs, FUN = function(x) length(x$test_set) ) )
```


```{r}
cv_parallel = function(i) {

  # get the test set and which day we evaluate on
  test_set = i$test_set
  test_day = i$test_day
  
  # test index if we leave out a given day/several timeIDs
  cv_index = ( df[[TIME_ID_NAME]] %in% test_set )  
  
  # get standardized rv dataframe, seperate into train and test datasets
  standardized_list = standardize(df, covariates = covariateNames, response = RESPONSE, lineIDname = LINE_ID_NAME, test_index = cv_index)
  
  # get training and testing sets
  train_df = standardized_list$train.df %>%
    mutate(!!TIME_ID_NAME := factor( !!sym(TIME_ID_NAME) ),
           !!LINE_ID_NAME := factor( !!sym(LINE_ID_NAME) )) %>%
    arrange(!!sym(LINE_ID_NAME), as.Date(!!sym(TIME_ID_NAME)))
  
  test_df = standardized_list$test.df %>%
    mutate(!!TIME_ID_NAME := factor(!!sym(TIME_ID_NAME)),
           !!LINE_ID_NAME := factor(!!sym(LINE_ID_NAME))) %>%
    arrange(!!sym(LINE_ID_NAME), as.Date(!!sym(TIME_ID_NAME)))
  
  # get group sizes
  group_sizes_train = train_df %>%
    group_by(!!sym(TIMEGROUP_ID_NAME)) %>%
    summarize(size = n()/L_) %>%
    pull(size)
  group_sizes = df %>%
    group_by(!!sym(TIMEGROUP_ID_NAME)) %>%
    summarize(size = n()/L_) %>%
    pull(size)
  
  # time fixed effects encoding matrix
  if (length(group_sizes_train) > 1) {
    S_time = cbind(
      bdiag(lapply(group_sizes_train, function(n) rep(1, n))) %*% contr.sum(length(group_sizes_train)),
      bdiag(lapply(group_sizes_train, function(n) contr.sum(n) ))
    )
  } else {
    S_time = contr.sum(group_sizes_train)
  }
  # time fixed effects design matrix
  X_train_time = kronecker(rep(1,L_), S_time)
  # design matrix for the line fixed effects
  X_train_line = kronecker( contr.sum(L_, sparse = T), rep(1,T_-length(test_set)) )
  # design matrix for the covariates
  X_train_covar = sparse.model.matrix(modelFormula,train_df) 
  # create full design matrix
  X_train = cbind(rep(1,nrow(X_train_time)),X_train_time,X_train_line,X_train_covar)
  
  rm(X_train_time, X_train_line, X_train_covar)
  
  # responses
  Y_train = train_df[[RESPONSE]]
    
  # fit the linear model using all data
  fit_lm = sparseLM(X_train, Y_train)
  
  # get the group-offset encoding for the test set test set based on time point
  g_t = (bdiag(lapply(group_sizes, function(n) rep(1, n))) %*% contr.sum(length(group_sizes)))[(timeIDs %in% test_set),]
  S_test_time = cbind(g_t, Matrix(0, nrow = length(test_set), ncol=sum(group_sizes_train) - length(group_sizes_train) ) )
  X_test_time = kronecker(rep(1,L_), S_test_time)
  
  # design matrix for the line fixed effects
  X_test_line = kronecker( contr.sum(L_, sparse = T), rep(1,length(test_set)) )
  # design matrix for the covariates
  X_test_covar = sparse.model.matrix(modelFormula,test_df) 
  # create full design matrix
  X_test = cbind(rep(1,nrow(X_test_covar)),X_test_time,X_test_line,X_test_covar)
  # responses
  Y_test = test_df[[RESPONSE]]
  
  rm(X_test_time, X_test_line, X_test_covar)
  
  # create a column for predicted rvs
  test_df[["pred_rv"]] = (X_test %*% fit_lm$beta_hat)[,1]


  # store data
  saveRDS(
    list(testDF = test_df %>%
           rename(contam_rv = !!sym(RESPONSE) ) %>%
           select(!!sym(TIME_ID_NAME), !!sym(LINE_ID_NAME), contam_rv, pred_rv),
         model_coefs = fit_lm$beta_hat[,1]),
    file = str_c(WD_DATA, "models/", MODEL_NAME, "/cv_block=",2*CV_NUM_DAY,"/cv_df_", test_day, ".rds" ) 
  )
  
}
```

```{r}
cv_parallel(left_out_timeIDs[[1]])
```

```{r}
cv_list <- pbmclapply(left_out_timeIDs, FUN = cv_parallel, mc.cores = 6)
#cv_df = do.call(rbind, cv_list)
rm(cv_list)
```



*playground*
```{r}
group_sizes_train = c(4,3,4)
if (length(group_sizes_train) > 1) {
  S_time_train = cbind(
    bdiag(lapply(group_sizes_train, function(n) rep(1, n))) %*% contr.sum(length(group_sizes_train)),
    bdiag(lapply(group_sizes_train, function(n) contr.sum(n) ))
  )
} else {
  S_time_train = contr.sum(group_sizes_train)
}
S_time_train

group_sizes = c(5,4,4)
if (length(group_sizes) > 1) {
  S_time = cbind(
    bdiag(lapply(group_sizes, function(n) rep(1, n))) %*% contr.sum(length(group_sizes)),
    bdiag(lapply(group_sizes, function(n) contr.sum(n) ))
  )
} else {
  S_time = contr.sum(group_sizes)
}
S_time
```

```{r}
test_set = c(5:6)
g_t = (bdiag(lapply(group_sizes, function(n) rep(1, n))) %*% contr.sum(length(group_sizes)))[test_set,]
g_t

cbind(g_t, Matrix(0, nrow = length(test_set), ncol=sum(group_sizes_train) - length(group_sizes_train) ) )
```


```{r}
group_sizes = c(5,4,3)
bdiag(lapply(group_sizes, function(n) rep(1, n))) %*% contr.sum(length(group_sizes))
# time fixed effects encoding matrix
if (length(group_sizes) > 1) {
  S_time = cbind(
    bdiag(lapply(group_sizes, function(n) rep(1, n))) %*% contr.sum(length(group_sizes)),
    bdiag(lapply(group_sizes, function(n) contr.sum(n) ))
  )
} else {
  S_time = contr.sum(group_sizes)
}
S_time
```

```{r}
group_sizes = c(5,3,3)
bdiag(lapply(group_sizes, function(n) rep(1, n))) %*% contr.sum(length(group_sizes))
# time fixed effects encoding matrix
if (length(group_sizes) > 1) {
  S_time = cbind(
    bdiag(lapply(group_sizes, function(n) rep(1, n))) %*% contr.sum(length(group_sizes)),
    bdiag(lapply(group_sizes, function(n) contr.sum(n) ))
  )
} else {
  S_time = contr.sum(group_sizes)
}
S_time
```



*testing CV function*
```{r}
i = left_out_timeIDs[[227]]

# get the test set and which day we evaluate on
test_set = i$test_set
test_day = i$test_day

# test index if we leave out a given day/several timeIDs
cv_index = ( df[[TIME_ID_NAME]] %in% test_set )  

# get standardized rv dataframe, seperate into train and test datasets
standardized_list = standardize(df, covariates = covariateNames, response = RESPONSE, lineIDname = LINE_ID_NAME, test_index = cv_index)
```

```{r}
# get training and testing sets
train_df = standardized_list$train.df %>%
  mutate(!!TIME_ID_NAME := factor( !!sym(TIME_ID_NAME) ),
         !!LINE_ID_NAME := factor( !!sym(LINE_ID_NAME) )) %>%
  arrange(!!sym(LINE_ID_NAME), as.Date(!!sym(TIME_ID_NAME)))

test_df = standardized_list$test.df %>%
  mutate(!!TIME_ID_NAME := factor(!!sym(TIME_ID_NAME)),
         !!LINE_ID_NAME := factor(!!sym(LINE_ID_NAME))) %>%
  arrange(!!sym(LINE_ID_NAME), as.Date(!!sym(TIME_ID_NAME)))

# get group sizes
group_sizes_train = train_df %>%
  group_by(!!sym(TIMEGROUP_ID_NAME)) %>%
  summarize(size = n()/L_) %>%
  pull(size)
group_sizes = df %>%
  group_by(!!sym(TIMEGROUP_ID_NAME)) %>%
  summarize(size = n()/L_) %>%
  pull(size)
```

get train design mat and fit model
```{r}
# time fixed effects encoding matrix
if (length(group_sizes_train) > 1) {
  S_time = cbind(
    bdiag(lapply(group_sizes_train, function(n) rep(1, n))) %*% contr.sum(length(group_sizes_train)),
    bdiag(lapply(group_sizes_train, function(n) contr.sum(n) ))
  )
} else {
  S_time = contr.sum(group_sizes_train)
}
# time fixed effects design matrix
X_train_time = kronecker(rep(1,L_), S_time)
# design matrix for the line fixed effects
X_train_line = kronecker( contr.sum(L_, sparse = T), rep(1,T_-length(test_set)) )
# design matrix for the covariates
X_train_covar = sparse.model.matrix(modelFormula,train_df) 
# create full design matrix
X_train = cbind(rep(1,nrow(X_train_time)),X_train_time,X_train_line,X_train_covar)

rm(X_train_time, X_train_line, X_train_covar)

# responses
Y_train = train_df[[RESPONSE]]
  
# fit the linear model using all data
fit_lm = sparseLM(X_train, Y_train)
```


```{r}
# get the group-offset encoding for the test set test set based on time point
g_t = (bdiag(lapply(group_sizes, function(n) rep(1, n))) %*% contr.sum(length(group_sizes)))[(timeIDs %in% test_set),]
S_test_time = cbind(g_t, Matrix(0, nrow = length(test_set), ncol=sum(group_sizes_train) - length(group_sizes_train) ) )
X_test_time = kronecker(rep(1,L_), S_test_time)

# design matrix for the line fixed effects
X_test_line = kronecker( contr.sum(L_, sparse = T), rep(1,length(test_set)) )
# design matrix for the covariates
X_test_covar = sparse.model.matrix(modelFormula,test_df) 
# create full design matrix
X_test = cbind(rep(1,nrow(X_test_covar)),X_test_time,X_test_line,X_test_covar)
# responses
Y_test = test_df[[RESPONSE]]

rm(X_test_time, X_test_line, X_test_covar)

# create a column for predicted rvs
test_df[["pred_rv"]] = (X_test %*% fit_lm$beta_hat)[,1]
```

```{r}
test_df %>%
  rename(contam_rv = !!sym(RESPONSE) ) %>%
  select(!!sym(TIME_ID_NAME), !!sym(LINE_ID_NAME), contam_rv, pred_rv) %>%
  summarize(rmse = sqrt(mean((contam_rv-pred_rv)^2)) )
```


*CV with LASSO*

```{r}
lasso_zero_columns = model_fit$lasso_zero_columns
```

```{r}
cv_parallel_LASSO = function(i) {
  # get the test set and which day we evaluate on
  test_set = i$test_set
  test_day = i$test_day
  
  # test index if we leave out a given day/several timeIDs
  cv_index = ( df[[TIME_ID_NAME]] %in% test_set )  
  
  # get standardized rv dataframe, seperate into train and test datasets
  standardized_list = standardize(df, covariates = covariateNames, response = RESPONSE, lineIDname = LINE_ID_NAME, test_index = cv_index)
  
  # get training and testing sets
  train_df = standardized_list$train.df %>%
    mutate(!!TIME_ID_NAME := factor( !!sym(TIME_ID_NAME) ),
           !!LINE_ID_NAME := factor( !!sym(LINE_ID_NAME) )) %>%
    arrange(!!sym(LINE_ID_NAME), as.Date(!!sym(TIME_ID_NAME)))
  
  test_df = standardized_list$test.df %>%
    mutate(!!TIME_ID_NAME := factor(!!sym(TIME_ID_NAME)),
           !!LINE_ID_NAME := factor(!!sym(LINE_ID_NAME))) %>%
    arrange(!!sym(LINE_ID_NAME), as.Date(!!sym(TIME_ID_NAME)))
  
  # get group sizes
  timeGroup_ids = train_df %>%
    dplyr::select(!!sym(TIMEGROUP_ID_NAME), !!sym(TIME_ID_NAME)) %>%
    unique()
  group_sizes = table(timeGroup_ids[[TIMEGROUP_ID_NAME]])
  
  # get which date groups are in the testing set
  test_df = test_df %>%
    mutate(!!TIMEGROUP_ID_NAME := factor(!!sym(TIMEGROUP_ID_NAME), levels = names(group_sizes) ) )
  
  # (sparse) design matrix for model, set contrasts
  X_train = sparse.model.matrix(modelFormula,
                                train_df,
                                contrasts = setNames(
                                  list(contr_groupSum(group_sizes), contr.sum),
                                  c(TIME_ID_NAME, LINE_ID_NAME)
                                )
  )
  
  
  # LASSO: remove zero columns from the design matrix
  X_train = X_train[,setdiff(colnames(X_train),lasso_zero_columns)]

  # responses
  Y_train = train_df[[RESPONSE]]
  
  # get every date in each group used to fit the model, this excludes the last date in each group (which should be sum2zero encoded)
  timeFE_columns = levels(train_df[[TIME_ID_NAME]])[-cumsum(group_sizes)]
  # append the group ID to the end of these columns
  timeFE_columns = str_c(timeFE_columns, "_group", rep( 1:length(group_sizes), group_sizes-1 ) )
  # rename columns of design matrix
  colnames(X_train)[1:sum(group_sizes)] = c("(Intercept)",
                                            if (length(group_sizes) > 1) {
                                              str_c(TIMEGROUP_ID_NAME,seq(1, length(group_sizes)-1))
                                            } else { NULL },
                                            timeFE_columns)
  
  print("finished making training set")
  
  # fit the linear model on the train data
  fit_train_lm = sparseLM(X_train, Y_train, PRINT_TIME = F)
  
  if ( is.null(fit_train_lm) ) {
    cat("cv failed, singular XtX\n")
    return()
  }
  
  print("finished fitting model")
  
  # (sparse) design matrix for model, without date
  X_test = sparse.model.matrix(
    update(modelFormula, as.formula(paste("~", TIMEGROUP_ID_NAME, "+ . - ",TIME_ID_NAME))),
    test_df,
    contrasts = setNames(list(contr.sum, contr.sum), c(TIMEGROUP_ID_NAME, LINE_ID_NAME)))  
  # responses
  Y_test = test_df[[RESPONSE]]
  
  # LASSO: remove zero columns from the design matrix
  X_test = X_test[,setdiff(colnames(X_test),lasso_zero_columns)]
  
  # get rid of model coefficients associated with date terms (alpha)
  model_coefs_nodate = fit_train_lm$beta_hat[  !grepl("^\\d{4}-\\d{2}-\\d{2}", rownames(fit_train_lm$beta_hat)), ]
  
  # make sure our design matrix and coefficients are the same
  if (!all( names(model_coefs_nodate) == colnames( X_test ) )) {
    cat("error in conforming matrices\n")
    return()
  }
  
  # create a column for predicted rvs
  test_df[["pred_rv"]] = (X_test %*% model_coefs_nodate)[,1] 
  
  # create a column for predicted rvs, subtracting mean RV intercept term (mu)
  #test_df[["pred_rv"]] = (X_test %*% model_coefs_nodate)[,1] - model_coefs_nodate[1]
  
  # store data
  saveRDS(
    list(testDF = test_df %>%
           rename(contam_rv = !!sym(RESPONSE) ) %>%
           select(!!sym(TIME_ID_NAME), !!sym(LINE_ID_NAME), contam_rv, pred_rv),
         model_coefs = model_coefs_nodate),
    file = str_c(WD_DATA, "models/", MODEL_NAME, "/cv/cv_df_", test_day, ".rds" ) 
  )
  
}
```

```{r}
cv_list <- pbmclapply(left_out_timeIDs, FUN = cv_parallel_LASSO, mc.cores = 8)
#cv_df = do.call(rbind, cv_list)
rm(cv_list)
```

# LASSO feature selection

```{r}
library(glmnet)
X = designMat
y = responses
```

```{r}
model_name = "LASSO"
model_dir = str_c(WD_DATA,"models/",model_name)
# create directory
if (!dir.exists(model_dir)) {dir.create(model_dir)}
```

```{r}
# set the penalty factors
penalty_factors = rep(1, ncol(X[,-1]))

# determine number of parameters to not penalize (time FE)
#non_penalized = T_ + 2*L_ - 3
non_penalized = (T_-1)+(L_-1)

# set the non-penalized columns
penalty_factors[1:non_penalized] = 0
```

```{r}
head( colnames(X[,-1])[1:non_penalized] )
tail( colnames(X[,-1])[1:non_penalized] )
```

```{r}
set.seed(123)
cv_lasso = cv.glmnet(X[,-1], y, alpha = 1, penalty.factor = penalty_factors)
#lasso_coef = coef( cv_lasso  )
```

```{r}
cv_lasso$lambda.min
cv_lasso$lambda.1se
```

```{r}
# lasso coefficients
lasso_coef = coef( glmnet(X[,-1], y, alpha = 1, lambda = 0.0002, penalty.factor = penalty_factors) )[,1]
```


```{r}
# get coefs that are exactly 0 in LASSO
zero_columns = (lasso_coef==0)
# remove those columns from the design matrix
designMat_LASSO = X[,!zero_columns]
# fit the linear model using all data
fit_lm = sparseLM(designMat_LASSO, y)

gamma = fit_lm$beta_hat[,1]
alpha = gamma[3:330]
cleanRV = (bdiag(lapply(group_sizes, contr.sum)) %*% alpha)[,1]
```

```{r}
# store results
fit_lm$AIC
fit_lm$BIC
sqrt( mean(( cleanRV )^2) )
fit_lm$RSE
sum(zero_columns)
```

lambda = 0.001497261
[1] 1041778
[1] 1130328
[1] 1.035963
[1] 7.483062
[1] 3530

lambda = 0.0002
[1] 995403.2
[1] 1109361
[1] 0.6217904
[1] 6.805841
[1] 1100

lambda = 0.0001
[1] 993426.2
[1] 1113124
[1] 0.5876584
[1] 6.772772
[1] 551

lambda = 0.00001
[1] 993752.4
[1] 1119107
[1] 0.5754356
[1] 6.770281
[1] 10

lambda = 0
[1] 993771.6
[1] 1119231
[1] 0.5754301
[1] 6.770408
[1] 0


```{r}
# initialize runs, lambdas, number of zero columns, AIC, BIC, and RMSE

num_runs = 40
lambdas = seq(0, 0.0002, length.out = num_runs)

numZeroCols = rep(NA,num_runs)
AICs = rep(NA,num_runs)
BICs = rep(NA,num_runs)
RMSEs = rep(NA,num_runs)
RSEs = rep(NA,num_runs)
```


```{r}
for (i in 1:length(lambdas)) {
  # set a lambda for our regularizer
  lam = lambdas[i]
  # lasso coefficients
  lasso_coef = coef( glmnet(X[,-1], y, alpha = 1, lambda = lam, penalty.factor = penalty_factors) )[,1]
  
  # get coefs that are exactly 0 in LASSO
  zero_columns = (lasso_coef==0)
  
  # store number of zero columns
  numZeroCols[i] = sum(zero_columns)
  
  # remove those columns from the design matrix
  designMat_LASSO = X[,!zero_columns]
  # fit the linear model using all data
  fit_lm = sparseLM(designMat_LASSO, y)

  gamma = fit_lm$beta_hat[,1]
  alpha = gamma[3:330]
  cleanRV = (bdiag(lapply(group_sizes, contr.sum)) %*% alpha)[,1]
  
  # store results
  AICs[i] = fit_lm$AIC
  BICs[i] = fit_lm$BIC
  RMSEs[i] = sqrt( mean(( cleanRV )^2) )
  RSEs[i] = fit_lm$RSE
}
```

```{r}
saveRDS(
  list(AICs = AICs, 
       BICs = BICs, 
       RMSEs = RMSEs,
       RSEs = RSEs,
       lambdas = lambdas,
       numZeroCols = numZeroCols),
  str_c(model_dir, "/LASSO_results.rds"))
```

## view LASSO results

```{r}
lasso_results = readRDS(str_c(model_dir, "/LASSO_results.rds"))
lasso_results
```

```{r}
ggplot() +
  geom_line(mapping = aes(x = lasso_results$numZeroCols, lasso_results$AICs)) +
  xlab("percent of columns removed") +
  ylab("AIC") +
  geom_vline(xintercept = lasso_results$numZeroCols[which.min(lasso_results$AICs)])+
  geom_vline(xintercept = lasso_results$numZeroCols[which.min(lasso_results$BICs)], color = "red")+
  geom_vline(xintercept = lasso_results$numZeroCols[which.min(lasso_results$RSEs)], color = "blue")
ggplot() +
  geom_line(mapping = aes(x = lasso_results$numZeroCols, lasso_results$BICs)) +
  xlab("percent of columns removed") +
  ylab("BIC") +
  geom_vline(xintercept = lasso_results$numZeroCols[which.min(lasso_results$AICs)])+
  geom_vline(xintercept = lasso_results$numZeroCols[which.min(lasso_results$BICs)], color = "red")+
  geom_vline(xintercept = lasso_results$numZeroCols[which.min(lasso_results$RSEs)], color = "blue")
ggplot() +
  geom_line(mapping = aes(x = lasso_results$numZeroCols, lasso_results$RMSEs)) +
  xlab("percent of columns removed") +
  ylab("RMSE") +
  geom_vline(xintercept = lasso_results$numZeroCols[which.min(lasso_results$AICs)])+
  geom_vline(xintercept = lasso_results$numZeroCols[which.min(lasso_results$BICs)], color = "red")+
  geom_vline(xintercept = lasso_results$numZeroCols[which.min(lasso_results$RSEs)], color = "blue")
ggplot() +
  geom_line(mapping = aes(x = lasso_results$numZeroCols, lasso_results$RSEs)) +
  xlab("percent of columns removed") +
  ylab("RSE")  +
  geom_vline(xintercept = lasso_results$numZeroCols[which.min(lasso_results$AICs)]) +
  geom_vline(xintercept = lasso_results$numZeroCols[which.min(lasso_results$BICs)], color = "red") +
  geom_vline(xintercept = lasso_results$numZeroCols[which.min(lasso_results$RSEs)], color = "blue")
```


```{r}
# full model (no lasso) RMSE
lasso_results$RMSEs[1]
```

```{r}
# RMSE differences between no LASSO vs min AIC LASSO
lasso_results$RMSEs[which.min(lasso_results$AICs)]
# how many columns are removed
lasso_results$numZeroCols[which.min(lasso_results$AICs)]
```

```{r}
# RMSE differences between no LASSO vs min BIC LASSO
lasso_results$RMSEs[which.min(lasso_results$BICs)]
# how many columns are removed
lasso_results$numZeroCols[which.min(lasso_results$BICs)]
```

```{r}
# RMSE differences between no LASSO vs min AIC LASSO
lasso_results$RMSEs[which.min(lasso_results$RSEs)]
# how many columns are removed
lasso_results$numZeroCols[which.min(lasso_results$RSEs)]
```

*fitting a linear model with the LASSO chosen features*

```{r}
# set a lambda for our regularizer via AIC
#lam = lasso_results$lambdas[which.min(lasso_results$AICs)]

# set a lambda for our regularizer via BIC
lam = lasso_results$lambdas[which.min(lasso_results$BICs)]
```

```{r}
# get coefs from a LASSO
lasso_coef = coef( glmnet(X[,-1], y, alpha = 1, lambda = lam, penalty.factor = penalty_factors) )
# get coefs that are exactly 0 in LASSO
zero_columns = rownames(lasso_coef)[(lasso_coef == 0)[,1]]
# remove those columns from the design matrix
designMat_LASSO = X[,setdiff(colnames(X),zero_columns)]
# fit the linear model using all data
fit_lm = sparseLM(designMat_LASSO, responses)
```

```{r}
# initialize 0's in the linear operator
linear_op_mat = Matrix(0, nrow = T_, ncol = length(fit_lm$beta_hat[,1]), sparse = T )
# matrix for estimating the cleaned RV
linear_op_mat[,(length(group_sizes)+1):sum(group_sizes)] = contr_groupSum(group_sizes)[,length(group_sizes):(sum(group_sizes)-1)]
# covariance matrix of model parameters
cov_mat = fit_lm$var_beta_hat

# dataframe of date's cleaned rv
cleanRV_df = data.frame(
  timeID = as.Date(timeIDs)
)
# find the mle estiamte, var, and se for the intercept plus alpha
cleanRV_df$estimate = (linear_op_mat %*% fit_lm$beta_hat[,1] )[,1]
cleanRV_covar = linear_op_mat %*% cov_mat %*% t(linear_op_mat)
cleanRV_df$var = diag( cleanRV_covar )
cleanRV_df$se = sqrt(cleanRV_df$var)
# find rmse of the clean RV
RMSE = rmse_t(0,cleanRV_df$estimate)
```

```{r}
RMSE
length(zero_columns)
```

```{r}
saveRDS(list(designMat = designMat_LASSO,
             responses = responses,
             df = rv_df,
             group_sizes = group_sizes,
             modelFormula = modelFormula,
             covariateNames = covariateNames,
             fit_lm = fit_lm,
             cleanRV_df = cleanRV_df,
             RMSE = RMSE,
             lasso_zero_columns = zero_columns),
        file = str_c(model_dir, "/model_BIC.rds" ) )
```

