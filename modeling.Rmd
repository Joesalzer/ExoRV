---
title: "NEID Solar Data -- Linear Modeling"
author: "Joe Salzer"
date: "`r Sys.Date()`"
output: html_document
---

This file is rstudio script for testing all of our modeling

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
source("readSpectra.R")
library(Matrix)
library(parallel)
library(pbmcapply)
```

# linear modeling

pass covariateNames to script
```{r}
covariateNames = c("fit_gauss_a", "fit_gauss_b", "fit_gauss_depth", "fit_gauss_sigmasq",str_c("proj_hg_coeff_", c(0,seq(2,10,1))))
# covariateNames = c("fit_gauss_a", "fit_gauss_b", "fit_gauss_depth", "fit_gauss_sigmasq",str_c("proj_thg_coeff_", c(0,seq(2,10,1))))
#covariateNames = c()

print(covariateNames)
```

fit_gauss_a,fit_gauss_b,fit_gauss_depth,fit_gauss_sigmasq,proj_hg_coeff_0,proj_hg_coeff_2,proj_hg_coeff_3,proj_hg_coeff_4,proj_hg_coeff_5,proj_hg_coeff_6,proj_hg_coeff_7,proj_hg_coeff_8,proj_hg_coeff_9,proj_hg_coeff_10

Rscript rv_lm.R completeLines.csv

```{r}
# get csv file name
csvFileName = "completeLines.csv"
```

## setting variables and loading data

variables to be set by the user

```{r}
# working directory with the data
WD_DATA = "/Users/josephsalzer/research/exostat/"
# RESPONSE variable
RESPONSE = "rv_template_0.5"
# names of the timeID, lineID, and timeGroupID
TIME_ID_NAME = "date"
LINE_ID_NAME = "line_order"
TIMEGROUP_ID_NAME = "date_groups"
# csv file name
completeLines_df = read_csv(str_c(WD_DATA,csvFileName)) %>%
  mutate(!!TIME_ID_NAME := as.Date( !!sym(TIME_ID_NAME) ))
```

## get list of lineIDs and timeIDs

```{r}
# vec of LineIDs  in completeLines_df
lineIDs = completeLines_df %>% group_by(!!sym(LINE_ID_NAME)) %>% summarize(n. = n()) %>% pull(!!sym(LINE_ID_NAME))
# vec of timeIDs in completeLines_df
timeIDs = completeLines_df %>% group_by(!!sym(TIME_ID_NAME)) %>% summarize(n. = n()) %>% pull(!!sym(TIME_ID_NAME))
T_ = length(timeIDs)
L_ = length(lineIDs)
```

## create model formula and subdirectories

the following chunk is the typical specification of the linear models as described in the paper and included in the script "rv_lm.R"

```{r}
# create directory called "models" in working directory
if (!dir.exists(str_c(WD_DATA,"models"))) {dir.create(str_c(WD_DATA,"models"))}

# base TWFE formula
twfe_formula = paste0("~ ",TIME_ID_NAME," + ",LINE_ID_NAME)

# if covariateNames aren't empty, include interactions, else use TWFE
if (!is_empty(covariateNames)) {
  # build interactions with LINE_ID_NAME for each provided centered covariate
  covar_slopes = paste0(LINE_ID_NAME,":", covariateNames, "_centered", collapse = " + ")
  # construct the full formula
  modelFormula = as.formula(paste(twfe_formula, covar_slopes, sep = " + "))
  
  # name the current model, using Gaussian fit parameters and hg covariateNames
  ## remove or edit these lines if using other covariateNames ##
  gaussCovars = covariateNames[startsWith(covariateNames,"fit_gauss")]
  gaussCovars_names = paste(str_split_i(gaussCovars, "_", i = 3),collapse=",")
  if (gaussCovars_names=="") {gaussCovars_names = "none"} else if(length(gaussCovars) == 4) {gaussCovars_names = "all"}
  hgCovars = covariateNames[startsWith(covariateNames,"proj_hg_coeff_")]
  hgCovars_names = paste(str_split_i(hgCovars, "_", i = 4),collapse=",")
  if (hgCovars_names=="") {hgCovars_names = "none"} else if(length(hgCovars) == 10) {hgCovars_names = "all"}
  model_name = str_c("Gauss=",gaussCovars_names,"_HG=",hgCovars_names)
  
  # create model directory
  model_dir = str_c(WD_DATA,"models/",model_name)
  if (!dir.exists(model_dir)) {dir.create(model_dir)}

  rm(twfe_formula,covar_slopes,gaussCovars,gaussCovars_names,hgCovars,hgCovars_names)
} else {
  # if empty, just fit TWFE
  modelFormula = as.formula(twfe_formula)
  # model name
  model_name = "TWFE"
  # create model directory
  model_dir = str_c(WD_DATA,"models/",model_name)
  if (!dir.exists(model_dir)) {dir.create(model_dir)}
  rm(twfe_formula)
}

cat("The model formula is:\n", as.character(modelFormula), "\n")
cat("The model name is:", model_name, "\n")
```

below are special types of the linear models: randomly assigned covariates, common slopes for covariateNames, and multiple intercept/slope per lineID/time group models

*random covars*
random variables
```{r}
set.seed(123)
completeLines_df[paste0("rand_col", 1:14)] = lapply(1:14, function(x) rnorm(nrow(completeLines_df)))
covariateNames = str_c("rand_col", 1:14)
covariateNames
```

```{r}
modelFormula = as.formula(~ date + line_order + line_order:rand_col1_centered + line_order:rand_col2_centered + line_order:rand_col3_centered + line_order:rand_col4_centered + line_order:rand_col5_centered + line_order:rand_col6_centered + line_order:rand_col7_centered + line_order:rand_col8_centered + line_order:rand_col9_centered + line_order:rand_col10_centered + line_order:rand_col11_centered + line_order:rand_col12_centered + line_order:rand_col13_centered + line_order:rand_col14_centered )

model_name = "random_covars"
model_dir = str_c(WD_DATA,"models/",model_name)
# create directory
if (!dir.exists(model_dir)) {dir.create(model_dir)}
```

*COMMON SLOPES*
```{r}
# covariateNames
covariateNames = c("fit_gauss_a", "fit_gauss_b", "fit_gauss_depth", "fit_gauss_sigmasq",str_c("proj_hg_coeff_", c(0,seq(2,10,1))))

modelFormula = as.formula(~ date + line_order + fit_gauss_a_centered + fit_gauss_b_centered + fit_gauss_depth_centered + fit_gauss_sigmasq_centered + proj_hg_coeff_0_centered + proj_hg_coeff_2_centered + proj_hg_coeff_3_centered + proj_hg_coeff_4_centered + proj_hg_coeff_5_centered + proj_hg_coeff_6_centered + proj_hg_coeff_7_centered + proj_hg_coeff_8_centered + proj_hg_coeff_9_centered + proj_hg_coeff_10_centered )
model_name = "CS_Gauss=all_HG=all"
model_dir = str_c(WD_DATA,"models/",model_name)

# create directory
if (!dir.exists(model_dir)) {dir.create(model_dir)}
```

*multiple intercept + slope model*
```{r}
# covariateNames
covariateNames = c("fit_gauss_a", "fit_gauss_b", "fit_gauss_depth", "fit_gauss_sigmasq",str_c("proj_hg_coeff_", c(0,seq(2,10,1))))

modelFormula = as.formula(~ date + lineOrderIntercept + lineOrderSlope:fit_gauss_a_centered + lineOrderSlope:fit_gauss_b_centered + lineOrderSlope:fit_gauss_depth_centered + lineOrderSlope:fit_gauss_sigmasq_centered + lineOrderSlope:proj_hg_coeff_0_centered + lineOrderSlope:proj_hg_coeff_2_centered + lineOrderSlope:proj_hg_coeff_3_centered + lineOrderSlope:proj_hg_coeff_4_centered + lineOrderSlope:proj_hg_coeff_5_centered + lineOrderSlope:proj_hg_coeff_6_centered + lineOrderSlope:proj_hg_coeff_7_centered + lineOrderSlope:proj_hg_coeff_8_centered + lineOrderSlope:proj_hg_coeff_9_centered + lineOrderSlope:proj_hg_coeff_10_centered  )
model_name = "all_MIMS"
model_dir = str_c(WD_DATA,"models/",model_name)

# create directory
if (!dir.exists(model_dir)) {dir.create(model_dir)}
```

## standardize dataframe

center by lineID, scaling each continuous column. We can "inject" a planet here, by changing the amplitude, frequency, and horizontal offset 

no planet
```{r}
# get standardized rv arranging by line_order and then date
rv_df = ( completeLines_df %>%
  standardize(covariates = covariateNames) )$train.df %>%
  arrange(!!sym(LINE_ID_NAME), as.Date(!!sym(TIME_ID_NAME)))
```

*with planet*
```{r}
AMP = 3
HORIZONTAL_OFFSET = 0
VERTICAL_OFFSET = 0
FREQ = 2*pi/250

# get standardized rv, with an injected planet, arranging by line_order and then date
rv_df = ( injectPlanet(completeLines_df, amp = AMP, freq = FREQ, horizontal_offset = HORIZONTAL_OFFSET, vertical_offset = VERTICAL_OFFSET) %>%
  standardize(covariates = covariateNames ) )$train.df %>%
  arrange(!!sym(LINE_ID_NAME), !!sym(TIME_ID_NAME))

# mean offset for non-noisy planet signal, noisy planet signal, and noise (original rv signal)
rv_df %>%
  select(date,date_groups,pert_val,pert_rv,rv_template_0.5) %>%
  unique() %>%
  group_by(date_groups) %>%
  summarize(planet_offset = mean(pert_val),
            noisePlanet_offset = mean(pert_rv),
            noise_offset = mean(rv_template_0.5))

estimated_rv = rv_df %>%
  group_by(date) %>%
  summarize(est_rv = mean(pert_rv))

seq_days = seq(min(estimated_rv$date), max(estimated_rv$date), by = 1)

ggplot() +
  geom_point(mapping = aes(x = estimated_rv$date, y = estimated_rv$est_rv)) +
  geom_line(mapping = aes(x = seq_days,
                          y = VERTICAL_OFFSET + AMP*sin(FREQ*changeFun(as.numeric(seq_days))+HORIZONTAL_OFFSET) ) )

rm(AMP,HORIZONTAL_OFFSET,VERTICAL_OFFSET,FREQ,estimated_rv,seq_days)
```


## set contrasts and make design matrix

*group sum2zero for timeIDs, sum2zero for lineIDs (single intercept, single slope model)*

```{r}
# create factors out of date + line_order
temp_df = rv_df %>%
    mutate(!!TIME_ID_NAME := factor( !!sym(TIME_ID_NAME) ),
           !!LINE_ID_NAME := factor( !!sym(LINE_ID_NAME) ))

# get group sizes
timeGroup_ids = temp_df %>%
  dplyr::select(!!sym(TIMEGROUP_ID_NAME), !!sym(TIME_ID_NAME)) %>%
  unique()
group_sizes = table(timeGroup_ids[[TIMEGROUP_ID_NAME]])


# (sparse) design matrix for model, set contrasts
designMat = sparse.model.matrix(
  modelFormula,
  temp_df,
  contrasts = setNames(
    list(contr_groupSum(group_sizes), contr.sum),
    c(TIME_ID_NAME, LINE_ID_NAME)
  )
)  

# get every date in each group used to fit the model, this excludes the last date in each group (which should be sum2zero encoded)
timeFE_columns = levels(temp_df[[TIME_ID_NAME]])[-cumsum(group_sizes)]
# append the group ID to the end of these columns
timeFE_columns = str_c(timeFE_columns, "_group", rep( 1:length(group_sizes), group_sizes-1 ) )
# rename columns of design matrix, assuming that the first T are the intercept and timeFE
colnames(designMat)[1:sum(group_sizes)] = c("(Intercept)",
                                            if (length(group_sizes) > 1) str_c(TIMEGROUP_ID_NAME,seq(1, length(group_sizes)-1)) else NULL,
                                            timeFE_columns)

# responses
responses = temp_df[[RESPONSE]]
```

```{r}
designMat[ c(1:4,227:230,T_:(T_+2)),c(1:4,229:230,T_:(T_+2))]
dim(designMat)
```

*group sum2zero for timeIDs, sum2zero for lineIDs (multiple intercept, multiple slope model)*

```{r}
# create factors out of date + line_order and lineOrderIntercept
temp_df = rv_df %>%
    mutate(!!TIME_ID_NAME := factor( !!sym(TIME_ID_NAME) ),
           !!LINE_ID_NAME := factor( !!sym(LINE_ID_NAME) ),
           lineOrderIntercept = factor(str_c(line_order,"_group",date_groups)),
           lineOrderSlope = factor(str_c(line_order,"_group",date_groups)))

# get group sizes
timeGroup_ids = temp_df %>%
  dplyr::select(!!sym(TIMEGROUP_ID_NAME), !!sym(TIME_ID_NAME)) %>%
  unique()
group_sizes = table(timeGroup_ids[[TIMEGROUP_ID_NAME]])

# (sparse) design matrix for model, set contrasts
designMat = sparse.model.matrix(
  modelFormula,
  temp_df,
  contrasts = setNames(
    list(contr_groupSum(group_sizes),lineGroupFE_contrast(group_sizes,L_),contr.sum),
    c(TIME_ID_NAME,"lineOrderIntercept","lineOrderSlope")
  )
)  

# get every date in each group used to fit the model, this excludes the last date in each group (which should be sum2zero encoded)
timeFE_columns = levels(temp_df[[TIME_ID_NAME]])[-cumsum(group_sizes)]
# append the group ID to the end of these columns
timeFE_columns = str_c(timeFE_columns, "_group", rep( 1:length(group_sizes), group_sizes-1 ) )
# rename columns of design matrix, assuming that the first T are the intercept and timeFE
colnames(designMat)[1:sum(group_sizes)] = c("(Intercept)",
                                            str_c(TIMEGROUP_ID_NAME,seq(1, length(group_sizes)-1) ),
                                            timeFE_columns)

# responses
responses = temp_df[[RESPONSE]]
```

```{r}
designMat[ c(1:5,227:230),c(1:5,229:230)]
dim(designMat)
```

*WITH PLANET*

```{r}
responses = rv_df$pert_rv
```

## fit model

```{r}
# fit the linear model using all data
fit_lm = sparseLM(designMat, responses)
```

## get time FE results

```{r}
# initialize 0's in the linear operator
linear_op_mat = Matrix(0, nrow = T_, ncol = length(fit_lm$beta_hat[,1]), sparse = T )
# matrix for estimating the cleaned RV
linear_op_mat[,(length(group_sizes)+1):sum(group_sizes)] = contr_groupSum(group_sizes)[,length(group_sizes):(sum(group_sizes)-1)]
# covariance matrix of model parameters
cov_mat = fit_lm$var_beta_hat

# dataframe of date's cleaned rv
cleanRV_df = data.frame(
  timeID = as.Date(timeIDs)
)
# find the mle estiamte, var, and se for the intercept plus alpha
cleanRV_df$estimate = (linear_op_mat %*% fit_lm$beta_hat[,1] )[,1]
cleanRV_covar = linear_op_mat %*% cov_mat %*% t(linear_op_mat)
cleanRV_df$var = diag( cleanRV_covar )
cleanRV_df$se = sqrt(cleanRV_df$var)
# find rmse of the clean RV
RMSE = rmse_t(0,cleanRV_df$estimate)
```

## save fit

```{r}
saveRDS(list(designMat = designMat,
             responses = responses,
             df = temp_df,
             timeGroup_ids = timeGroup_ids,
             modelFormula = modelFormula,
             covariateNames = covariateNames,
             fit_lm = fit_lm,
             cleanRV_df = cleanRV_df,
             RMSE = RMSE),
        file = str_c(model_dir, "/model.rds" ) )
```

# leverage calculations

```{r}
# arguments passed to command line
# model name
MODEL_NAME = "TWFE"
BATCH_SIZES = 1000
```

```{r}
# working directory with the data
WD_DATA = "/Users/josephsalzer/research/exostat/"
# read in the model fit
model_fit = readRDS(str_c(WD_DATA, "models/", MODEL_NAME, "/model.rds"))
fit_lm = model_fit$fit_lm
X = model_fit$designMat
XtX_inv = (fit_lm$var_beta_hat)/fit_lm$sigma2_hat
```

```{r}
# directory containing the leverage files
leverage_dir = str_c(WD_DATA, "models/", MODEL_NAME, "/leverages")
# create directory called "leverages" in model name
if ( !dir.exists(leverage_dir) ) {
  dir.create(leverage_dir)
}
```

```{r}
# optimized leverage computation
compute_leverage_batches = function(indices) {
  start_row = indices[1]
  end_row = indices[2]
  
  # extract rows in a batch
  rows = X[start_row:end_row, , drop = FALSE]
  # compute leverage for all rows in the batch
  leverage = rowSums(rows * (rows %*% XtX_inv))
  
  names(leverage) = str_c("row",start_row:end_row)

  # save results in a single file for the batch
  batch_filename = file.path(leverage_dir, str_c("leverage_", start_row, "_", end_row, ".rds"))
  saveRDS(leverage, file = batch_filename)
}

# function to create batches
create_batches = function(n_rows, batch_size) {
  # start indices of batches
  starts = seq(1, n_rows, by = batch_size)
  # end indices of batches
  ends = pmin(starts + batch_size - 1, n_rows)
  # combine into a list of start and end indices
  batches = Map(c, starts, ends)
  return(batches)
}
```

```{r}
# create batch indices
batches = create_batches(nrow(X), 1000)
```

```{r}
# test
#compute_leverage_batches(batches[[2]])
```

```{r}
# parallel computation of leverage
result <- pbmclapply(batches, FUN = compute_leverage_batches, mc.cores = 7)
```

```{r}
# get the list of all .rds files in the leverage directory
leverage_files = list.files(path = leverage_dir, pattern = "\\.rds$", full.names = TRUE)
# initialize an empty vector to store the combined leverage values
leverages = vector()
for (file in leverage_files) {
  leverages = c(leverages, readRDS(file))
}
rm(leverage_files)
```

```{r}
# add leverages to model_fit
model_fit$leverages = leverages
saveRDS(model_fit, file = str_c(WD_DATA, "models/", MODEL_NAME, "/model.rds" ) )
```

# bootstrap coefficients and residuals

```{r}
# arguments passed to command line
# model name
MODEL_NAME = "Gauss=all_HG=all"
NUM_BOOTS = 500
```

```{r}
# working directory with the data
WD_DATA = "/Users/josephsalzer/research/exostat/"
# names of the timeID, lineID, and timeGroupID
TIME_ID_NAME = "date"
LINE_ID_NAME = "line_order"
```

```{r}
# directory containing the bootstrap files
bootstrap_dir = str_c(WD_DATA, "models/", MODEL_NAME, "/wild_bootstraps" )
# create directory called "bootstrap" in model name
if ( !dir.exists(bootstrap_dir) ) {
  dir.create(bootstrap_dir)
}
```

```{r}
# read in the model fit
model_fit = readRDS(str_c(WD_DATA, "models/", MODEL_NAME, "/model.rds"))
# lm fit, df, design matrix and leverages
fit_lm = model_fit$fit_lm
rv_df = model_fit$df
designMat = model_fit$designMat
leverages = model_fit$leverages

# vec of LineIDs  in completeLines_df
lineIDs = rv_df %>% group_by(!!sym(LINE_ID_NAME)) %>% summarize(n. = n()) %>% pull(!!sym(LINE_ID_NAME))
# vec of timeIDs in completeLines_df
timeIDs = as.Date(rv_df %>% group_by(!!sym(TIME_ID_NAME)) %>% summarize(n. = n()) %>% pull(!!sym(TIME_ID_NAME)))
  
T_ = length(timeIDs)
L_ = length(lineIDs)
```

```{r}
# get modified residuals
u_hat = fit_lm$resid/(1-leverages)
# get unmodified residuals
#u_hat = fit_lm$resid
```

assign blocks for the bootstrap
```{r}
# assign blocks for the bootstrap (by line)

block_size = length(timeIDs)

rv_df = rv_df %>%
  arrange(!!sym(LINE_ID_NAME),!!sym(TIME_ID_NAME)) %>%
  mutate(boot_block = factor( ceiling(row_number() / block_size) ) )
```

```{r}
# # assign blocks for the bootstrap (by line's depth)
# 
# block_size = length(lineIDs)/15
# 
# blockByLine = rv_df %>%
#   group_by(line_order) %>%
#   summarize(mean_depth = mean(fit_gauss_depth)) %>%
#   arrange(mean_depth) %>%
#   mutate(boot_block = factor( ceiling(row_number() / block_size) ) )
# 
# rv_df = rv_df %>%
#   merge(blockByLine, by = "line_order")
# 
# rm(blockByLine)
```

```{r}
# # assign blocks for the bootstrap (by day)
# 
# block_size = 3*length(lineIDs)
# 
# rv_df = rv_df %>%
#   arrange(date, line_order) %>%
#   mutate(boot_block = factor( ceiling(row_number() / block_size) ) ) %>%
#   arrange(line_order, date)
```

```{r}
wildBoot_parallel = function(seedID) {
  
  # set a given seed for replication
  set.seed(seedID)
  
  # ensure that the rv_df is arranged first by lineID and then timeID, and that these are factors
  rv_df = rv_df %>%
    arrange(!!sym(LINE_ID_NAME), !!sym(TIME_ID_NAME)) %>%
    mutate( lineID = factor(!!sym(LINE_ID_NAME)),
            timeID = factor(!!sym(TIME_ID_NAME)))
  
  # rademacher random variable for each block in the dataset
  v = sample( c(-1, 1), size = length(unique(rv_df$boot_block)), replace = TRUE)
  # generate wild bootstrap residuals by block
  u_star = u_hat*v[rv_df$boot_block]
  
  # construct bootstrap sample
  y_star = fit_lm$y_hat + u_star
  # re-estimate the model on bootstrap sample
  fitstar_lm = sparseLM(designMat, y_star, PRINT_TIME = F)

  # store data
  saveRDS(fitstar_lm$beta_hat[,1],
          file = str_c(bootstrap_dir, "/bootstraps_list_", seedID, ".rds" ) )
  
}
```

run wild bootstrap

```{r}
boot_straps <- pbmclapply(100*( c(1:NUM_BOOTS) + 2000 ), FUN = wildBoot_parallel, mc.cores = 7)
rm(boot_straps)
```

# cross validation

```{r}
# pass on command line

# model name
MODEL_NAME = "LASSO"
# number of timeIDs in each leave-one-out cv
CV_NUM_DAY = 28
```

*replace response with planet*
```{r}
# working directory with the data
WD_DATA = "/Users/josephsalzer/research/exostat/"
# RESPONSE variable
RESPONSE = "rv_template_0.5"
# names of the timeID, lineID, and timeGroupID
TIME_ID_NAME = "date"
LINE_ID_NAME = "line_order"
TIMEGROUP_ID_NAME = "date_groups"
# read in the model fit
model_fit = readRDS(str_c(WD_DATA, "models/", MODEL_NAME, "/model.rds" ))
df = model_fit$df
covariateNames = model_fit$covariateNames
modelFormula = model_fit$modelFormula
```

```{r}
# vec of LineIDs  in completeLines_df
lineIDs = df %>% group_by(!!sym(LINE_ID_NAME)) %>% summarize(n. = n()) %>% pull(!!sym(LINE_ID_NAME))
# vec of timeIDs in completeLines_df
timeIDs = as.Date(df %>% group_by(!!sym(TIME_ID_NAME)) %>% summarize(n. = n()) %>% pull(!!sym(TIME_ID_NAME)))
  
T_ = length(timeIDs)
L_ = length(lineIDs)
```

```{r}
# create directory called "cv" in working directory
if ( !dir.exists(str_c(WD_DATA,"models/",MODEL_NAME,"/cv")) ) {
  dir.create(str_c(WD_DATA,"models/",MODEL_NAME,"/cv"))
}
```

```{r}
# list of timeIDs to be left out
left_out_timeIDs = create_sliding_windows(timeIDs, CV_NUM_DAY)
left_out_timeIDs[[1]]
left_out_timeIDs[[2]]
left_out_timeIDs[[15]]
left_out_timeIDs[[25]]
```

```{r}
cv_parallel = function(i) {
  # get the test set and which day we evaluate on
  test_set = i$test_set
  test_day = i$test_day
  
  # test index if we leave out a given day/several timeIDs
  cv_index = ( df[[TIME_ID_NAME]] %in% test_set )  
  
  # get standardized rv dataframe, seperate into train and test datasets
  standardized_list = standardize(df, covariates = covariateNames, response = RESPONSE, lineIDname = LINE_ID_NAME, test_index = cv_index)

  # get training and testing sets
  train_df = standardized_list$train.df %>%
    mutate(!!TIME_ID_NAME := factor( !!sym(TIME_ID_NAME) ),
           !!LINE_ID_NAME := factor( !!sym(LINE_ID_NAME) )) %>%
    arrange(!!sym(LINE_ID_NAME), as.Date(!!sym(TIME_ID_NAME)))

  test_df = standardized_list$test.df %>%
    mutate(!!TIME_ID_NAME := factor(!!sym(TIME_ID_NAME)),
           !!LINE_ID_NAME := factor(!!sym(LINE_ID_NAME))) %>%
    arrange(!!sym(LINE_ID_NAME), as.Date(!!sym(TIME_ID_NAME)))

  
  # get group sizes
  timeGroup_ids = train_df %>%
    dplyr::select(!!sym(TIMEGROUP_ID_NAME), !!sym(TIME_ID_NAME)) %>%
    unique()
  group_sizes = table(timeGroup_ids[[TIMEGROUP_ID_NAME]])
  
  # get which date groups are in the testing set
  test_df = test_df %>%
    mutate(!!TIMEGROUP_ID_NAME := factor(!!sym(TIMEGROUP_ID_NAME), levels = names(group_sizes) ) )
  # (sparse) design matrix for model, set contrasts
  X_train = sparse.model.matrix(modelFormula,
                                train_df,
                                contrasts = setNames(
                                  list(contr_groupSum(group_sizes), contr.sum),
                                  c(TIME_ID_NAME, LINE_ID_NAME)
                                )
  )  
  # responses
  Y_train = train_df[[RESPONSE]]
  
  # get every date in each group used to fit the model, this excludes the last date in each group (which should be sum2zero encoded)
  timeFE_columns = levels(train_df[[TIME_ID_NAME]])[-cumsum(group_sizes)]
  # append the group ID to the end of these columns
  timeFE_columns = str_c(timeFE_columns, "_group", rep( 1:length(group_sizes), group_sizes-1 ) )
  # rename columns of design matrix
  colnames(X_train)[1:sum(group_sizes)] = c("(Intercept)",
                                            if (length(group_sizes) > 1) {
                                              str_c(TIMEGROUP_ID_NAME,seq(1, length(group_sizes)-1))
                                            } else { NULL },
                                            timeFE_columns)
  
  print("finished making training set")
  
  # fit the linear model on the train data
  fit_train_lm = sparseLM(X_train, Y_train, PRINT_TIME = F)
  
  if ( is.null(fit_train_lm) ) {
    cat("cv failed, singular XtX\n")
    return()
  }
  
  print("finished fitting model")
  
  # (sparse) design matrix for model, without date
  X_test = sparse.model.matrix(
    update(modelFormula, as.formula(paste("~", TIMEGROUP_ID_NAME, "+ . - ",TIME_ID_NAME))),
    test_df,
    contrasts = setNames(list(contr.sum, contr.sum), c(TIMEGROUP_ID_NAME, LINE_ID_NAME)))  
  # responses
  Y_test = test_df[[RESPONSE]]
  
  # get rid of model coefficients associated with date terms (alpha)
  model_coefs_nodate = fit_train_lm$beta_hat[  !grepl("^\\d{4}-\\d{2}-\\d{2}", rownames(fit_train_lm$beta_hat)), ]
  
  # make sure our design matrix and coefficients are the same
  if (!all( names(model_coefs_nodate) == colnames( X_test ) )) {
    cat("error in conforming matrices\n")
    return()
  }
  
  # create a column for predicted rvs
  test_df[["pred_rv"]] = (X_test %*% model_coefs_nodate)[,1] 
  
  # create a column for predicted rvs, subtracting mean RV intercept term (mu)
  #test_df[["pred_rv"]] = (X_test %*% model_coefs_nodate)[,1] - model_coefs_nodate[1]
  
  # store data
  saveRDS(
    list(testDF = test_df %>%
           rename(contam_rv = !!sym(RESPONSE) ) %>%
           select(!!sym(TIME_ID_NAME), !!sym(LINE_ID_NAME), contam_rv, pred_rv),
         model_coefs = model_coefs_nodate),
    file = str_c(WD_DATA, "models/", MODEL_NAME, "/cv/cv_df_", test_day, ".rds" ) 
  )
  
}
```

```{r}
cv_list <- pbmclapply(left_out_timeIDs, FUN = cv_parallel, mc.cores = 8)
#cv_df = do.call(rbind, cv_list)
rm(cv_list)
```

*CV with LASSO*

```{r}
lasso_zero_columns = model_fit$lasso_zero_columns
```

```{r}
cv_parallel_LASSO = function(i) {
  # get the test set and which day we evaluate on
  test_set = i$test_set
  test_day = i$test_day
  
  # test index if we leave out a given day/several timeIDs
  cv_index = ( df[[TIME_ID_NAME]] %in% test_set )  
  
  # get standardized rv dataframe, seperate into train and test datasets
  standardized_list = standardize(df, covariates = covariateNames, response = RESPONSE, lineIDname = LINE_ID_NAME, test_index = cv_index)
  
  # get training and testing sets
  train_df = standardized_list$train.df %>%
    mutate(!!TIME_ID_NAME := factor( !!sym(TIME_ID_NAME) ),
           !!LINE_ID_NAME := factor( !!sym(LINE_ID_NAME) )) %>%
    arrange(!!sym(LINE_ID_NAME), as.Date(!!sym(TIME_ID_NAME)))
  
  test_df = standardized_list$test.df %>%
    mutate(!!TIME_ID_NAME := factor(!!sym(TIME_ID_NAME)),
           !!LINE_ID_NAME := factor(!!sym(LINE_ID_NAME))) %>%
    arrange(!!sym(LINE_ID_NAME), as.Date(!!sym(TIME_ID_NAME)))
  
  # get group sizes
  timeGroup_ids = train_df %>%
    dplyr::select(!!sym(TIMEGROUP_ID_NAME), !!sym(TIME_ID_NAME)) %>%
    unique()
  group_sizes = table(timeGroup_ids[[TIMEGROUP_ID_NAME]])
  
  # get which date groups are in the testing set
  test_df = test_df %>%
    mutate(!!TIMEGROUP_ID_NAME := factor(!!sym(TIMEGROUP_ID_NAME), levels = names(group_sizes) ) )
  
  # (sparse) design matrix for model, set contrasts
  X_train = sparse.model.matrix(modelFormula,
                                train_df,
                                contrasts = setNames(
                                  list(contr_groupSum(group_sizes), contr.sum),
                                  c(TIME_ID_NAME, LINE_ID_NAME)
                                )
  )
  
  
  # LASSO: remove zero columns from the design matrix
  X_train = X_train[,setdiff(colnames(X_train),lasso_zero_columns)]

  # responses
  Y_train = train_df[[RESPONSE]]
  
  # get every date in each group used to fit the model, this excludes the last date in each group (which should be sum2zero encoded)
  timeFE_columns = levels(train_df[[TIME_ID_NAME]])[-cumsum(group_sizes)]
  # append the group ID to the end of these columns
  timeFE_columns = str_c(timeFE_columns, "_group", rep( 1:length(group_sizes), group_sizes-1 ) )
  # rename columns of design matrix
  colnames(X_train)[1:sum(group_sizes)] = c("(Intercept)",
                                            if (length(group_sizes) > 1) {
                                              str_c(TIMEGROUP_ID_NAME,seq(1, length(group_sizes)-1))
                                            } else { NULL },
                                            timeFE_columns)
  
  print("finished making training set")
  
  # fit the linear model on the train data
  fit_train_lm = sparseLM(X_train, Y_train, PRINT_TIME = F)
  
  if ( is.null(fit_train_lm) ) {
    cat("cv failed, singular XtX\n")
    return()
  }
  
  print("finished fitting model")
  
  # (sparse) design matrix for model, without date
  X_test = sparse.model.matrix(
    update(modelFormula, as.formula(paste("~", TIMEGROUP_ID_NAME, "+ . - ",TIME_ID_NAME))),
    test_df,
    contrasts = setNames(list(contr.sum, contr.sum), c(TIMEGROUP_ID_NAME, LINE_ID_NAME)))  
  # responses
  Y_test = test_df[[RESPONSE]]
  
  # LASSO: remove zero columns from the design matrix
  X_test = X_test[,setdiff(colnames(X_test),lasso_zero_columns)]
  
  # get rid of model coefficients associated with date terms (alpha)
  model_coefs_nodate = fit_train_lm$beta_hat[  !grepl("^\\d{4}-\\d{2}-\\d{2}", rownames(fit_train_lm$beta_hat)), ]
  
  # make sure our design matrix and coefficients are the same
  if (!all( names(model_coefs_nodate) == colnames( X_test ) )) {
    cat("error in conforming matrices\n")
    return()
  }
  
  # create a column for predicted rvs
  test_df[["pred_rv"]] = (X_test %*% model_coefs_nodate)[,1] 
  
  # create a column for predicted rvs, subtracting mean RV intercept term (mu)
  #test_df[["pred_rv"]] = (X_test %*% model_coefs_nodate)[,1] - model_coefs_nodate[1]
  
  # store data
  saveRDS(
    list(testDF = test_df %>%
           rename(contam_rv = !!sym(RESPONSE) ) %>%
           select(!!sym(TIME_ID_NAME), !!sym(LINE_ID_NAME), contam_rv, pred_rv),
         model_coefs = model_coefs_nodate),
    file = str_c(WD_DATA, "models/", MODEL_NAME, "/cv/cv_df_", test_day, ".rds" ) 
  )
  
}
```

```{r}
cv_list <- pbmclapply(left_out_timeIDs, FUN = cv_parallel_LASSO, mc.cores = 8)
#cv_df = do.call(rbind, cv_list)
rm(cv_list)
```

# LASSO feature selection

inputs (and group_sizes)
```{r}
library(glmnet)
X = designMat
y = responses
```

```{r}
model_name = "LASSO"
model_dir = str_c(WD_DATA,"models/",model_name)
# create directory
if (!dir.exists(model_dir)) {dir.create(model_dir)}
```

```{r}
# set the penalty factors
penalty_factors = rep(1, ncol(X[,-1]))
# determine number of parameters to not penalize (intercept, time FE, line FE)
non_penalized = T_ + L_ - 2
# set the non-penalized columns
penalty_factors[1:non_penalized] = 0

# check to make sure correct columns are/arent penalized!
head(colnames(X[,-1])[1:non_penalized])
tail(colnames(X[,-1])[1:non_penalized], 13)
```

```{r}
# initialize runs, lambdas, number of zero columns, AIC, BIC, and RMSE
num_runs = 25
lambdas = seq(0.00021, 0.0004, length.out = num_runs)
numZeroCols = rep(NA,num_runs)
AICs = rep(NA,num_runs)
BICs = rep(NA,num_runs)
RMSEs = rep(NA,num_runs)
RSEs = rep(NA,num_runs)
```

```{r}
for (i in 1:length(lambdas)) {
  # set a lambda for our regularizer
  lam = lambdas[i]
  # get coefs from a LASSO
  lasso_coef = coef( glmnet(X[,-1], y, alpha = 1, lambda = lam, penalty.factor = penalty_factors) )
  # get coefs that are exactly 0 in LASSO
  zero_columns = rownames(lasso_coef)[(lasso_coef == 0)[,1]]
  
  # store number of zero columns
  numZeroCols[i] = length(zero_columns)
  
  # remove those columns from the design matrix
  designMat_LASSO = X[,setdiff(colnames(X),zero_columns)]
  # fit the linear model using all data
  fit_lm = sparseLM(designMat_LASSO, y)
  # initialize 0's in the linear operator
  linear_op_mat = Matrix(0, nrow = T_, ncol = length(fit_lm$beta_hat[,1]), sparse = T )
  # matrix for estimating the cleaned RV
  linear_op_mat[,(length(group_sizes)+1):sum(group_sizes)] = contr_groupSum(group_sizes)[,length(group_sizes):(sum(group_sizes)-1)]
  # dataframe of date's cleaned rv
  cleanRV_df = data.frame(
    date = as.Date(timeIDs)
  )
  # find the mle estimate, var, and se for the intercept plus alpha
  cleanRV_df$estimate = linear_op_mat %*% fit_lm$beta_hat[,1]
  
  # store results
  AICs[i] = fit_lm$AIC
  BICs[i] = fit_lm$BIC
  RMSEs[i] = sqrt( mean(( cleanRV_df$estimate )^2) )
  RSEs[i] = fit_lm$RSE
}
```


## view LASSO results
```{r}
lasso_results = readRDS(str_c(model_dir, "/LASSO_results.rds"))
#lasso_results
```

```{r}
ggplot() +
  geom_line(mapping = aes(x = 100*lasso_results$numZeroCols/(L_*14), lasso_results$AICs)) +
  xlab("percent of columns removed") +
  ylab("AIC") +
  geom_vline(xintercept = 100*lasso_results$numZeroCols[which.min(lasso_results$AICs)]/(L_*14),
             linetype = 2) +
  geom_vline(xintercept = 100*lasso_results$numZeroCols[which.min(lasso_results$RSEs)]/(L_*14),
             linetype = 2, color = "red")
ggplot() +
  geom_line(mapping = aes(x = 100*lasso_results$numZeroCols/(L_*14), lasso_results$BICs)) +
  xlab("percent of columns removed") +
  ylab("BIC") +
  geom_vline(xintercept = 100*lasso_results$numZeroCols[which.min(lasso_results$AICs)]/(L_*14),
             linetype = 2)+
  geom_vline(xintercept = 100*lasso_results$numZeroCols[which.min(lasso_results$RSEs)]/(L_*14),
             linetype = 2, color = "red")
ggplot() +
  geom_line(mapping = aes(x = 100*lasso_results$numZeroCols/(L_*14), lasso_results$RMSEs)) +
  xlab("percent of columns removed") +
  ylab("RMSE") +
  geom_vline(xintercept = 100*lasso_results$numZeroCols[which.min(lasso_results$AICs)]/(L_*14),
             linetype = 2)+
  geom_vline(xintercept = 100*lasso_results$numZeroCols[which.min(lasso_results$RSEs)]/(L_*14),
             linetype = 2, color = "red")
ggplot() +
  geom_line(mapping = aes(x = 100*lasso_results$numZeroCols/(L_*14), lasso_results$RSEs)) +
  xlab("percent of columns removed") +
  ylab("RSE") +
  geom_vline(xintercept = 100*lasso_results$numZeroCols[which.min(lasso_results$AICs)]/(L_*14),
             linetype = 2)+
  geom_vline(xintercept = 100*lasso_results$numZeroCols[which.min(lasso_results$RSEs)]/(L_*14),
             linetype = 2, color = "red")
```

```{r}
# RMSE differences between no LASSO vs min AIC LASSO
lasso_results$RMSEs[1]
lasso_results$RMSEs[which.min(lasso_results$AICs)]
# percent difference
100*(lasso_results$RMSEs[which.min(lasso_results$AICs)] - lasso_results$RMSEs[1])/lasso_results$RMSEs[1]
# how many columns are removed
lasso_results$numZeroCols[which.min(lasso_results$AICs)]
100*lasso_results$numZeroCols[which.min(lasso_results$AICs)]/(L_*14)
```

```{r}
# which lambda minimized the AIC
lasso_results$lambdas[which.min(lasso_results$AICs)]
```

*fitting a linear model with the LASSO chosen features*

```{r}
# set a lambda for our regularizer
lam = lasso_results$lambdas[which.min(lasso_results$AICs)]
```

```{r}
# get coefs from a LASSO
lasso_coef = coef( glmnet(X[,-1], y, alpha = 1, lambda = lam, penalty.factor = penalty_factors) )
# get coefs that are exactly 0 in LASSO
zero_columns = rownames(lasso_coef)[(lasso_coef == 0)[,1]]
# remove those columns from the design matrix
designMat_LASSO = X[,setdiff(colnames(X),zero_columns)]
# fit the linear model using all data
fit_lm = sparseLM(designMat_LASSO, responses)
```

```{r}
# initialize 0's in the linear operator
linear_op_mat = Matrix(0, nrow = T_, ncol = length(fit_lm$beta_hat[,1]), sparse = T )
# matrix for estimating the cleaned RV
linear_op_mat[,(length(group_sizes)+1):sum(group_sizes)] = contr_groupSum(group_sizes)[,length(group_sizes):(sum(group_sizes)-1)]
# covariance matrix of model parameters
cov_mat = fit_lm$var_beta_hat

# dataframe of date's cleaned rv
cleanRV_df = data.frame(
  timeID = as.Date(timeIDs)
)
# find the mle estiamte, var, and se for the intercept plus alpha
cleanRV_df$estimate = (linear_op_mat %*% fit_lm$beta_hat[,1] )[,1]
cleanRV_covar = linear_op_mat %*% cov_mat %*% t(linear_op_mat)
cleanRV_df$var = diag( cleanRV_covar )
cleanRV_df$se = sqrt(cleanRV_df$var)
# find rmse of the clean RV
RMSE = rmse_t(0,cleanRV_df$estimate)
```

```{r}
saveRDS(list(designMat = designMat_LASSO,
             responses = responses,
             df = temp_df,
             timeGroup_ids = timeGroup_ids,
             modelFormula = modelFormula,
             covariateNames = covariateNames,
             fit_lm = fit_lm,
             cleanRV_df = cleanRV_df,
             RMSE = RMSE,
             lasso_zero_columns = zero_columns),
        file = str_c(model_dir, "/model.rds" ) )
```


