---
title: "NEID Solar Data -- Cross validation"
author: "Joe Salzer"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(stringr)
library(lme4)
library(MASS)
library(gridExtra)
library(car)
library(emmeans)
library(glmnet)
```

```{r}
## Data directory
wd_data = "/Users/josephsalzer/research/exostat/"
```

```{r}
## Data directory
wd_data = "/Users/josephsalzer/research/exostat/"
```

# Loading data

```{r}
# get species info for each line
species.df = read_csv(str_c(wd_data,"neid_solar/","clean_line_info.csv")) %>%
  dplyr::select(lambda, species, excitation_energy) %>%
  mutate(lambda = factor( round(lambda,3) ),
         species = factor(species)) %>%
  unique()
```


```{r}
# convert categorical vars to factors
model.df = tibble( read.csv(str_c(wd_data,"neid_solar/", "final_fits_6kmps.csv") ) ) %>%
  rename( wavelength = lambda ) %>%
  mutate( obs_date = factor(obs_date),
          lambda = factor(wavelength),
          order_idx = factor(order_idx),
          lam_order = factor( lam_order )
        )
```

```{r}
# join final fits and species
model.df = model.df %>%
  left_join(species.df, by = "lambda")
```

```{r}
rm(species.df)
```

```{r}
head(model.df)
```

```{r}
# list of lambdas in model.df
final_lams = ( model.df %>% group_by(lambda) %>% summarize(n. = n()) )$lambda
length(final_lams)
```
```{r}
# list of lambda-orders in model.df
final_lams_orders = ( model.df %>% group_by(lam_order) %>% summarize(n. = n()) )$lam_order
length(final_lams_orders)
```

```{r}
# list of lambda-orders in model.df
final_days = ( model.df %>% group_by(obs_date) %>% summarize(n. = n()) )$obs_date
length(final_days)
```

# Data summaries

```{r}
# check how many line-order pairs there are
model.df %>%
  group_by(lambda, order_idx) %>%
  count()
```
there are 310 line-order pairs, 146 observations per line-order pair


# function to standardize our data after train/test split

```{r}
# columns that we do not want to apply standardization to
col.notstandard = 
  c("lambda", "wavelength", "lam_c1", "order_idx","repeat_order","lam_order","obs_date",
  "snr","mean_snr","is_max_snr", "species", "excitation_energy")
col.notstandard
```

```{r}
# columns that we want to apply standardization to
col.standard = colnames(model.df)[
  !(colnames(model.df) %in% col.notstandard) & !endsWith(colnames(model.df),"center") & !endsWith(colnames(model.df),"scale") & !startsWith(colnames(model.df),"thg") & !startsWith(colnames(model.df),"sigma")
]
col.standard
```

```{r}
# reduce the columns of our model
model.df = model.df %>%
  dplyr::select( all_of( c(col.notstandard,col.standard) ) )
model.df
```

```{r}
# center and scale function
centerFun = function(x, na.rm = FALSE) (x - mean(x, na.rm = na.rm))
scaleFun = function(x, na.rm = FALSE) (x - mean(x, na.rm = na.rm)) / sd(x, na.rm)
```

We have to be careful when we remove values from our dataset. Our centering and scaling must be over the training set, and then the mean and sd of those datasets must be applied to the testing set.

*train_test_standardize*
df: dataframe to seperate into training and testing, with centering and scaling over the training set
testIndex : index for left out days, integers
toScale : whether we center and scale or just center
```{r}
train_test_standardize = function(df, testIndex, toScale = F) {
  
  # create training data without a specific date
  train = model.df %>% filter( !(obs_date %in% final_days[testIndex]) )
  # create testing data with the specific date
  test = model.df %>% filter( obs_date %in% final_days[testIndex] )
  
  # means of our covariates for the trained days
  train.means = train %>%
    group_by(lam_order) %>%
    summarize_at(col.standard,mean)
  # sds of our covariates for the trained days
  train.sds = train %>%
    group_by(lam_order) %>%
    summarize_at(col.standard,sd)
  
  if (toScale) {
    print("not yet")
  } else {
    
    # for columns to be standardized, subtract out the line-specific mean
    test[,col.standard] = test[,col.standard] - train.means[,-1]
    
    # center the training set
    train = train %>%
      group_by(lam_order) %>%
      mutate_at(col.standard,centerFun) %>%
      ungroup()
  }
  
  return(list(train,test))
}
```

## test above function

```{r}
train_test_standardize(model.df,100)
# create training data without a specific date
train = model.df %>% filter( !(obs_date %in% final_days[100]) )
# create testing data with the specific date
test = model.df %>% filter( obs_date %in% final_days[100] )
```

check testing set rv:
```{r}
head( test$rv-( train %>% group_by(lam_order) %>% summarize(m = mean(rv)) )$m )
```
compare to if we standardized before train/test split

```{r}
head( test$rv-( model.df %>% group_by(lam_order) %>% summarize(m = mean(rv)) )$m )
```

```{r}
rm(train,test)
```

# Cross-validation

### 80/20 split (center rv model)

```{r}
set.seed(1234)

# sample size
n = length(model.df$rv)

# creating training data as 80% of the dataset
random_sample = sample( c( rep(T, round(.8*n) ), rep(F, round(.2*n) ) ) )
```

```{r}
# generating training dataset
train = model.df[random_sample, ]
# generating testing dataset
test = model.df[!random_sample, ]
```

```{r}
# linear model on training data
lm.train = lm(rv_center ~ obs_date + hg_0_scale + hg_2_scale + hg_3_scale + hg_4_scale + hg_5_scale + hg_6_scale + hg_7_scale + hg_8_scale + hg_9_scale + hg_10_scale + a1_scale + depth1_scale + width1_scale + b1_scale, train)
```


```{r}
# linear model on training data
lm.train = lm(rv_center ~ obs_date + hg_0_center + hg_2_center + hg_3_center + hg_4_center + hg_5_center + hg_6_center + hg_7_center + hg_8_center + hg_9_center + hg_10_center + a1_center + depth1_center + width1_center + b1_center, train)
```

```{r}
# add column of predicted rv to test dataframe
test$pred_rv_center = predict(lm.train, newdata = test)
```

```{r}
test %>%
  ggplot(mapping = aes(x = pred_rv_center, y = rv_center, color = snr)) +
  geom_point() +
  geom_abline(slope=1,intercept=0,linetype = 2)
```
```{r}
sqrt( mean( (test$pred_rv_center - test$rv_center)^2 ) )
mean( abs(test$pred_rv_center - test$rv_center) )
```

```{r}
rm(random_sample, train, test, lm.train, n )
```

### 80/20 split (scale rv model)

```{r}
set.seed(1234)

# sample size
n = length(model.df$rv)

# creating training data as 80% of the dataset
random_sample = sample( c( rep(T, round(.8*n) ), rep(F, round(.2*n) ) ) )
```

```{r}
# generating training dataset
train = model.df[random_sample, ]
# generating testing dataset
test = model.df[!random_sample, ]
```

```{r}
# linear model on training data
lm.train = lm(rv_scale ~ obs_date + hg_0_scale + hg_2_scale + hg_3_scale + hg_4_scale + hg_5_scale + hg_6_scale + hg_7_scale + hg_8_scale + hg_9_scale + hg_10_scale + a1_scale + depth1_scale + width1_scale + b1_scale, train)
```

```{r}
# add column of predicted rv to test dataframe
test$pred_rv_scale = predict(lm.train, newdata = test)
```

```{r}
test %>%
  ggplot(mapping = aes(x = pred_rv_scale, y = rv_scale, color = snr)) +
  geom_point() +
  geom_abline(slope=1,intercept=0,linetype = 2)
```

```{r}
sqrt( mean( (test$pred_rv_scale - test$rv_scale)^2 ) )
mean( abs(test$pred_rv_scale - test$rv_scale) )
```

```{r}
rm(random_sample, train, test, lm.train, n)
```


### leave-one-day out (centered rv model) for every date perturbation

```{r}
# amplitude
amp = 2
# frequency
freq = 1/30
```

```{r}
# create perturbed df where each day is perturbed by a sinusodial wave
# date_num is the obsdate converted to a numerical and divided to decrease freq of perturbation
# sin_date is 1 times the sin of date_num
pert.df = scaledmodel.df %>%
  group_by(obs_date) %>%
  mutate(date_num = as.numeric(as.Date(obs_date)),
         pert_val = amp*sin(freq*date_num)) %>%
  ungroup() %>%
  mutate(rv_pert = rv + pert_val) %>%
  dplyr::select(!c(centered_rv, scale_rv)) %>%
  # we ought to redo our centering and scaling after we add the perturbation
  group_by(lam_order) %>%
  mutate( centered_rv_pert = rv_pert - mean(rv_pert),
          scale_rv_pert = centered_rv_pert/sd(rv_pert)) %>%
  ungroup()
pert.df
```

```{r}
# initialize list of dfs
df.list = list()
```


BEGIN FOR LOOP

```{r}
for (day.num in 1:length(final_days) ) {
  
  # create training data without a specific date
  train = pert.df %>% filter(obs_date != final_days[day.num])
  # create testing data with the specific date
  test = pert.df %>% filter(obs_date == final_days[day.num])
  
  # linear model on training data
  lm.train = lm(centered_rv_pert ~ lam_order + obs_date + b1 + gh_3 + gh_2 + gh_5 + 
    gh_7 + depth1 + gh_9 + a1 + snr + gh_11 + gh_4 + width1 + 
    gh_0 + gh_10, train )
  
  # get the data matrix of our test dataset
  Xtest = model.matrix(centered_rv_pert ~ lam_order + b1 + gh_3 + gh_2 + gh_5 + 
    gh_7 + depth1 + gh_9 + a1 + snr + gh_11 + gh_4 + width1 + 
    gh_0 + gh_10, test)
  
  # get rid of model coefficients associated with obs_date
  model_coefs = lm.train$coefficients[ !startsWith( names(lm.train$coefficients), "obs_date") ]
  model_coefs[is.na(model_coefs)] = 0
  
  # check if we can multiply our coefficients and data matrix to get predictions
  print( all( names(model_coefs) == colnames(Xtest) ) )
  print( day.num )
  
  # add dataframe of predicted rvs, true rvs, and snrs to df.list
  df.list[[day.num]] = data.frame(
    pred_rv = Xtest %*% model_coefs,
    lam_order = test$lam_order,
    obs_date = test$obs_date
    )
  
  rm(train,test,lm.train,Xtest,model_coefs)
}
```

```{r}
# bind all dataframes in df.list to pred.df
pred.df = do.call(rbind, df.list)
pred.df
```

```{r}
# save pred.df
write.csv(pred.df, str_c(wd_data, "cv_pred_pert.csv"), row.names=FALSE)
```

```{r}
rm(df.list, pred.df, amp, freq)
```

#### Prediction errors

```{r}
# read pred.df
pred.df = read.csv(str_c(wd_data, "cv_pred_pert.csv"))
```

```{r}
pert.df = pert.df %>%
  inner_join(pred.df %>% dplyr::select( !c(true_rv, snr) ), by = c("lam_order","obs_date"))
```


predicted rv by true rv (colored by snr)

```{r}
pert.df %>%
  ggplot(mapping = aes(x = pred_rv, y = centered_rv_pert, color = snr )) +
  geom_point(alpha = .4) +
  geom_abline(slope=1,intercept=0,linetype = 2)
```
```{r}
pert.df %>%
  ggplot(mapping = aes(x = pred_rv, y = centered_rv_pert, color = snr )) +
  geom_point(alpha = .4) +
  geom_abline(slope=1,intercept=0,linetype = 2) +
  xlim(-40,40) +
  ylim(-40,40)
```

abs difference by snr

lam_order
```{r}
pert.df %>%
  ggplot(mapping = aes(x = snr, y = abs(pred_rv-centered_rv_pert) ) ) +
  geom_point()
```

```{r}
pert.df %>%
  ggplot() +
  geom_point(mapping = aes(x = as.Date(obs_date), y = pert_val), color = "red") +
  xlab("day")
```

```{r}
sqrt( mean( (pert.df$centered_rv_pert - pert.df$pred_rv)^2 ) )
mean( abs(pert.df$centered_rv_pert - pert.df$pred_rv) )
```

#### Tails of prediction errors

```{r}
pred.df = pred.df %>% mutate(pred_error = true_rv - pred_rv)
```

```{r}
# quantiles of errors
err.quantiles = quantile(pred.df$pred_error, probs = c(.05, .25, .5, .75, .95))
err.quantiles
```

```{r}
# dataframe of tails (5% and 95%) of predictions
tail.df = pred.df %>% 
  filter(pred_error < err.quantiles[1] | pred_error > err.quantiles[5]) %>%
  mutate(lambda = factor(lambda))
```

```{r}
tail.df
```

```{r}
length( unique(tail.df$lambda_order) )
length( unique(tail.df$obs_date) )
```

? (out of 310) line-orders appear in the tail of the distribution
every date appears in the tail of the distribution

Bar graph of how many times a specific line appears in the tails of the prediction error distribution
```{r}
tail.df %>%
  count(lam_order) %>%
  ggplot(aes(x = lam_order, y = n)) +
  geom_bar(stat="identity") +
  theme(axis.text.x = element_blank(),
        axis.ticks = element_blank() )
```

Which lambdas appear in tail more than 150 times:
```{r}
bad_lams = ( tail.df %>% group_by(lam_order) %>% summarize(n = n()) %>% filter(n > 150) )$lam_order
bad_lams 
```

```{r}
tail.df %>%
  group_by(lam_order) %>%
  summarize(
            min = min(snr),
            max = max(snr),
            median = median(snr),
            mean = mean(snr))
```

snr distribution by whether it is in tail or not

```{r}
pred.df %>%
  mutate( isTail = if_else(pred_error < err.quantiles[1] | pred_error > err.quantiles[5], T, F)  ) %>%
  ggplot(mapping = aes(x = snr, color = isTail)) +
  geom_density()
```

```{r}
print(bad_lams)
```

### leave-one-day out (lambda x b1 interaction model) (non-centered rv model) for every date

```{r}
# amplitude
amp = 2
# frequency
freq = 1/30
```

```{r}
# create perturbed df where each day is perturbed by a sinusodial wave
# date_num is the obsdate converted to a numerical and divided to decrease freq of perturbation
# sin_date is 1 times the sin of date_num
pert.df = scaledmodel.df %>%
  group_by(obs_date) %>%
  mutate(date_num = as.numeric(as.Date(obs_date)),
         pert_val = amp*sin(freq*date_num)) %>%
  ungroup() %>%
  mutate(rv_pert = rv + pert_val) %>%
  dplyr::select(!c(centered_rv, scale_rv)) %>%
  # we ought to redo our centering and scaling after we add the perturbation
  group_by(lam_order) %>%
  mutate( centered_rv_pert = rv_pert - mean(rv_pert),
          scale_rv_pert = centered_rv_pert/sd(rv_pert)) %>%
  ungroup()
pert.df
```

```{r}
# initialize our prediction data
pred.df = tibble( pred_rv = c(), 
                  true_rv = c(),
                  lambda = c(),
                  obs_date = c(),
                  order_idx = c(),
                  snr = c() )
pred.df
```

```{r}
# list of obs_date in scaledmodel.df
final_days = ( scaledmodel.df %>% group_by(obs_date) %>% summarize(n. = n()) )$obs_date
```

```{r}
scaledmodel.df = scaledmodel.df %>% mutate(lambda = droplevels(lambda))
```

BEGIN FOR LOOP

```{r}
for (day in final_days) {
  # create training data without a specific date
  train = scaledmodel.df %>% filter(obs_date != day) %>% drop_na()
  # create testing data with the specific date
  test = scaledmodel.df %>% filter(obs_date == day) %>% drop_na()
  
  # linear model on training data
  lm.train = lm(centered_rv ~ lambda*b1 + order_idx + obs_date + b1 + gh_3 + gh_2 + gh_5 + 
    gh_7 + depth1 + gh_9 + a1 + snr + gh_11 + gh_4 + width1 + 
    gh_0 + gh_10, train )
  
  # get the data matrix of our test dataset
  Xtest = model.matrix(centered_rv ~ lambda*b1 + order_idx + b1 + gh_3 + gh_2 + gh_5 + 
    gh_7 + depth1 + gh_9 + a1 + snr + gh_11 + gh_4 + width1 + 
    gh_0 + gh_10, test)
  
  # get rid of model coefficients associated with obs_date
  model_coefs = lm.train$coefficients[ !startsWith( names(lm.train$coefficients), "obs_date") ]
  model_coefs[is.na(model_coefs)] = 0
  
  # check if we can multiply our coefficients and data matrix to get predictions
  print(all( names(model_coefs) == colnames(Xtest) ))
  print(day)
  
  # create temp dataframe with predicted rv, true rv, lambda, obs date, order, and original snr
  temp = tibble( pred_rv = Xtest %*% model_coefs, 
                 true_rv = test$centered_rv,
                 lambda = test$lambda,
                 obs_date = test$obs_date,
                 order_idx = test$order_idx,
                 snr = ( model_df %>% filter(obs_date == day) %>% drop_na() )$snr )
  
  # combine temp dataframe and the full prediction dataframe
  pred.df = rbind(pred.df, temp )
  
  # remove temp dataframe
  rm(temp)
  
}
```

```{r}
# save pred.df
write.csv(pred.df, str_c(wd_data, "cv_pred.csv"), row.names=FALSE)
```

```{r}
# read pred.df
pred.df = read.csv(str_c(wd_data, "neid_solar/cv_pred.csv"))
```

```{r}
pred.df
```


```{r}
#png(filename="crossval_lodo.png", width=1000, height=500)
pred.df %>%
  ggplot(mapping = aes(x = pred_rv, y = true_rv, color = order_idx )) +
  geom_point() +
  geom_abline(slope=1,intercept=0,linetype = 2)
#dev.off()
```

```{r}
# snr*(predicted_rv - actual_rv) histogram
pred.df %>%
  ggplot(mapping = aes(x = snr*(pred_rv-true_rv))) +
  geom_histogram()
```

predicted rv by snr

```{r}
pred.df %>%
  ggplot(mapping = aes(x = snr, y = abs(pred_rv-true_rv), color = as.numeric(lambda)) ) +
  geom_point()
```

```{r}
sqrt( mean( (pred.df$true_rv - pred.df$pred_rv)^2 ) )
mean( abs(pred.df$true_rv - pred.df$pred_rv) )
```


#### Tails of prediction errors

```{r}
pred.df = pred.df %>% mutate(pred_error = true_rv - pred_rv)
```

```{r}
# quantiles of errors
err.quantiles = quantile(pred.df$pred_error, probs = c(.05, .25, .5, .75, .95))
err.quantiles
```

```{r}
# dataframe of tails (5% and 95%) of predictions
tail.df = pred.df %>% 
  filter(pred_error < err.quantiles[1] | pred_error > err.quantiles[5]) %>%
  mutate(lambda = factor(lambda))
```

```{r}
tail.df
```


```{r}
length( unique(tail.df$lambda) )
length( unique(tail.df$order_idx) )
length( unique(tail.df$obs_date) )
```

133 (out of 245) lines appear in the tail of the distribution
63 (out of 70) orders appear in the tail of the distribution
every date appears in the tail of the distribution

Bar graph of how many times a specific line appears in the tails of the prediction error distribution
```{r}
tail.df %>%
  count(lambda) %>%
  ggplot(aes(x = lambda, y = n)) +
  geom_bar(stat="identity") +
  theme(axis.text.x = element_blank(),
        axis.ticks = element_blank() )
```

Which lambdas appear in tail more than 150 times:
```{r}
bad_lams = ( tail.df %>% group_by(lambda) %>% summarize(n = n()) %>% filter(n > 150) )$lambda
bad_lams 
```

```{r}
tail.df %>%
  group_by(lambda,order_idx) %>%
  summarize(
            min = min(snr),
            max = max(snr),
            median = median(snr),
            mean = mean(snr))
```

snr distribution by whether it is in tail or not

```{r}
pred.df %>%
  mutate( isTail = if_else(pred_error < err.quantiles[1] | pred_error > err.quantiles[5], T, F)  ) %>%
  ggplot(mapping = aes(x = snr, color = isTail)) +
  geom_density()
```


```{r}
print(bad_lams)
```
4294.734 4459.485 4584.393 4845.84  4888.541


#### Influential lines

Let's remove the 4 "bad" lambdas and see how much we improve our model:

```{r}
lm.cv = lm(rv ~ lambda*b1 + order_idx + obs_date + b1 + gh_3 + gh_2 + gh_5 + 
    gh_7 + depth1 + gh_9 + a1 + snr + gh_11 + gh_4 + width1 + 
    gh_0 + gh_10, scaledmodel.df %>% filter( !(lambda %in% bad_lams) ))
```

```{r}
summary(lm.cv)$sigma
```

*compared to 4.876027*

```{r}
lm.cv = lm(centered_rv ~ lambda*b1 + order_idx + obs_date + b1 + gh_3 + gh_2 + gh_5 + 
    gh_7 + depth1 + gh_9 + a1 + snr + gh_11 + gh_4 + width1 + 
    gh_0 + gh_10, scaledmodel.df %>% filter( !(lambda %in% bad_lams) ))
```

```{r}
summary(lm.cv)$sigma
```
*compared to 4.421432*

```{r}
# dataframe of coefficients
coef.df = tibble( coef = rownames(summary(lm.cv)$coefficients),
                  estimate = summary(lm.cv)$coefficients[,1],
                  se = summary(lm.cv)$coefficients[,2],
                  pval = summary(lm.cv)$coefficients[,4] )
```


```{r}
coef.df %>% filter( startsWith( coef, "obs_date") ) %>%
  ggplot(mapping = aes(x = coef)) +
  geom_point(aes(y = estimate)) +
  theme(axis.text.x = element_blank(), axis.ticks = element_blank() ) +
  geom_errorbar(aes(ymin = estimate - 1.96*se, ymax = estimate + 1.96*se)) +
  geom_hline(yintercept = 0, color = "red")
```
```{r}
coef.df %>% 
  filter( startsWith( coef, "lambda") ) %>%
  filter( !endsWith( coef, "b1") )
```


```{r}
coef.df %>% 
  filter( startsWith( coef, "lambda") ) %>%
  filter( !endsWith( coef, "b1") ) %>%
  ggplot(mapping = aes(x = coef)) +
  geom_point(aes(y = estimate)) +
  theme(axis.text.x = element_blank(), axis.ticks = element_blank() ) +
  #geom_errorbar(aes(ymin = estimate - 1.96*se, ymax = estimate + 1.96*se)) +
  geom_hline(yintercept = 0, color = "red")
```

```{r}
set.seed(456)

# sample size
n = length( (scaledmodel.df %>% filter( !(lambda %in% bad_lams) ))$rv)
# creating training data as 80% of the dataset
random_sample = sample( c( rep(T, round(.8*n) ), rep(F, round(.2*n) ) ) )

# generating training dataset
train = (scaledmodel.df %>% filter( !(lambda %in% bad_lams) ))[random_sample, ]

# generating testing dataset
test = (scaledmodel.df %>% filter( !(lambda %in% bad_lams) ))[!random_sample, ]

# linear model on training data
lm.train = lm(centered_rv ~ lambda*b1 + order_idx + obs_date + gh_3 + gh_2 + gh_5 + 
    gh_7 + depth1 + gh_9 + a1 + snr + gh_11 + gh_4 + width1 + 
    gh_0 + gh_10, train )

# add column of predicted rv to test dataframe
test$pred_rv = predict(lm.train, newdata = test)
# add columns of prediction errors
test$predSE = (test$pred_rv-test$centered_rv)^2
test$predAE = abs(test$pred_rv-test$centered_rv)
```

```{r}
sqrt( mean( (test$centered_rv - test$pred_rv)^2 ) )
mean( abs(test$centered_rv - test$pred_rv) )
```

*compared ~roughly~ to 4.749467 and 2.437717*

```{r}
#png(filename="crossval_8020.png", width=1000, height=500)
test %>%
  ggplot(mapping = aes(x = pred_rv, y = centered_rv, color = order_idx)) +
  geom_point() +
  geom_abline(slope=1,intercept=0,linetype = 2)
#dev.off()
```

```{r}
rm(pred.df, test, train, Xtest, day.num, model_coefs, lm.train)
```


### leave-one-line out (single line)

```{r}
line.num = 2
```

```{r}
final_lams[line.num]
```

```{r}
# create training data without a specific lambda
train = scaledmodel.df %>% filter(lambda != final_lams[line.num])
# create testing data with the specific lambda
test = scaledmodel.df %>% filter(lambda == final_lams[line.num])
```

```{r}
test
```

```{r}
# linear model on training data
lm.train = lm(rv ~ lambda + order_idx + obs_date + b1 + gh_3 + gh_2 + gh_5 + 
    gh_7 + depth1 + gh_9 + a1 + snr + gh_11 + gh_4 + width1 + 
    gh_0 + gh_10, train )
```

```{r}
# create a model matrix of our testing data without lambda
Xtest = model.matrix(rv ~ order_idx + obs_date + b1 + gh_3 + gh_2 + gh_5 + 
    gh_7 + depth1 + gh_9 + a1 + snr + gh_11 + gh_4 + width1 + 
    gh_0 + gh_10, test)
```

```{r}
# get rid of model coefficients associated with obs_date
model_coefs = lm.train$coefficients[ !startsWith( names(lm.train$coefficients), "lambda") ]
model_coefs[is.na(model_coefs)] = 0
```

```{r}
# check that model coefs and column names of our data matrix match
all( names(model_coefs) == colnames(Xtest) )
```

```{r}
# create dataframe of predicted rv, true rv, and original (unscaled snr)
pred.df = tibble( pred_rv = Xtest %*% model_coefs, 
                  true_rv = test$rv, 
                  snr = (scaledmodel.df %>% filter(lambda == final_lams[line.num]) )$snr,
                  order_idx = test$order_idx)
pred.df
```

```{r}
pred.df %>%
  ggplot(mapping = aes(x = pred_rv, y = true_rv, color = order_idx)) +
  geom_point() +
  geom_abline(slope=1,intercept=0,linetype = 2)
```

```{r}
# snr*(predicted_rv - actual_rv) histogram
pred.df %>%
  ggplot(mapping = aes(x = snr*(pred_rv-true_rv))) +
  geom_histogram()
```


```{r}
sqrt( mean( (pred.df$true_rv - pred.df$pred_rv)^2 ) )
mean( abs(pred.df$true_rv - pred.df$pred_rv) )
```

```{r}
rm(lm.train, pred.df, test, train, Xtest, line.num, model_coefs)
```

### old analysis

```{r}
# old dataframe, where we just standardized every column w/o regards to line
old.df = model.df %>%
  mutate_at(col.standard,centerFun)
```

cv:
```{r}
set.seed(1234)

# sample size
n = length(old.df$rv)

# creating training data as 80% of the dataset
random_sample = sample( c( rep(T, round(.8*n) ), rep(F, round(.2*n) ) ) )
```

```{r}
# generating training dataset
train = old.df[random_sample, ]
# generating testing dataset
test = old.df[!random_sample, ]
```

```{r}
# linear model on training data
lm.train = lm(rv_center ~ obs_date + hg_0 + hg_2 + hg_3 + hg_4 + hg_5 + hg_6 + hg_7 + hg_8 + hg_9 + hg_10 + hg_11 + a1 + depth1 + width1 + b1, train)
```

```{r}
# add column of predicted rv to test dataframe
test$pred_rv_center = predict(lm.train, newdata = test)
```

```{r}
test %>%
  ggplot(mapping = aes(x = pred_rv_center, y = rv_center, color = snr)) +
  geom_point() +
  geom_abline(slope=1,intercept=0,linetype = 2)
```


```{r}
sqrt( mean( (test$pred_rv_center - test$rv_center)^2 ) )
mean( abs(test$pred_rv_center - test$rv_center) )
```

no cv
```{r}
# the current linear model to be worked on
currentlm = lm(rv_center ~ lam_order + obs_date + hg_0 + hg_2 + hg_3 + hg_4 + hg_5 + hg_6 + hg_7 + hg_8 + hg_9 + hg_10 + hg_11 + a1 + depth1 + width1 + b1, data=old.df) 
```

```{r}
# get emmeans 
emmean.summary = summary( emmeans(currentlm, "obs_date", nuisance = "lam_order") )
```

```{r}
# dataframe of coefficients of currentlm 
coef.df = tibble( coef = rownames(summary(currentlm )$coefficients),
                  estimate = summary(currentlm )$coefficients[,1],
                  se = summary(currentlm )$coefficients[,2],
                  pval = summary(currentlm )$coefficients[,4] )
```

```{r}
# adj.r.squared
summary(currentlm)$adj.r.squared
# RSE
summary(currentlm)$sigma
# standard error of date effect (mean)
mean(emmean.summary$SE)
# percent of days that aren't statistically different than 0 (95%)
mean(
  (emmean.summary$emmean - qnorm(.975)*emmean.summary$SE < 0) & (emmean.summary$emmean + qnorm(.975)*emmean.summary$SE > 0)
)
# percent of days aren't statistically different than 0 (99%)
mean(
  (emmean.summary$emmean - qnorm(0.995)*emmean.summary$SE < 0) & (emmean.summary$emmean + qnorm(0.995)*emmean.summary$SE > 0)
)
```

old model, centering shape measurements:
[1] 0.8286145
[1] 4.79946
[1] 0.2728433
[1] 0.9383562
[1] 0.9863014

new model, centering shape measurements:
[1] 0.8297888
[1] 4.78299
[1] 0.271907
[1] 0.9383562
[1] 0.9863014

```{r}
old.df %>%
  ggplot(mapping = aes(x = rv_center, y = predict(currentlm), color = snr)) +
  geom_point() +
  geom_abline(slope=1,intercept=0,linetype = 2)
```

```{r}
rm(old.df, coef.df, currentlm, emmean.summary)
```

### leave-one-day out (with perturbation by day)

```{r}
# amplitude
amp = 2
# frequency
freq = 1/30
```

```{r}
# create perturbed df where each day is perturbed by a sinusodial wave
# date_num is the obsdate converted to a numerical and divided to decrease freq of perturbation
# sin_date is 1 times the sin of date_num
pert.df = model.df %>%
  group_by(obs_date) %>%
  mutate(date_num = as.numeric(as.Date(obs_date)),
         pert_val = amp*sin(freq*date_num)) %>%
  ungroup() %>%
  mutate(rv_pert_center = rv_center + pert_val,
         rv_pert_scale = rv_scale + pert_val)
pert.df
```

```{r}
# graph of perturbation
pert.df %>%
  dplyr::select(date_num, pert_val) %>%
  unique() %>%
  ggplot() +
  geom_function(fun = function(x) amp*sin(freq*x), color = "red" ) +
  geom_point(mapping = aes(x = date_num, y = pert_val))
```


```{r}
# initialize list of dfs
df.list = list()
```

```{r}
day.num = 50
final_days[day.num]
```

```{r}
# perturbation for left-out day
unique(pert.df$pert_val)[day.num]
```

```{r}
# create training data without a specific date
train = pert.df %>% filter(obs_date != final_days[day.num])
# create testing data with the specific date
test = pert.df %>% filter(obs_date == final_days[day.num])
```

center:
```{r}
# linear model on training data
lm.train = lm(rv_pert_center ~ 0 + obs_date + hg_0_scale + hg_2_scale + hg_3_scale + hg_4_scale + hg_5_scale + hg_6_scale + hg_7_scale + hg_8_scale + hg_9_scale + hg_10_scale + hg_11_scale + a1_scale + depth1_scale + width1_scale + b1_scale, train )
```

```{r}
#summary(lm.train)
```

```{r}
# create a model matrix of our testing data without a specific day
Xtest = model.matrix(rv_pert_center ~ 0 + hg_0_scale + hg_2_scale + hg_3_scale + hg_4_scale + hg_5_scale + hg_6_scale + hg_7_scale + hg_8_scale + hg_9_scale + hg_10_scale + hg_11_scale + a1_scale + depth1_scale + width1_scale + b1_scale, test)
```

scale
```{r}
# linear model on training data
lm.train = lm(rv_pert_scale ~ obs_date + hg_0_scale + hg_2_scale + hg_3_scale + hg_4_scale + hg_5_scale + hg_6_scale + hg_7_scale + hg_8_scale + hg_9_scale + hg_10_scale + hg_11_scale + a1_scale + depth1_scale + width1_scale + b1_scale, train )
```

```{r}
# create a model matrix of our testing data without a specific day
Xtest = model.matrix(rv_pert_scale ~ hg_0_scale + hg_2_scale + hg_3_scale + hg_4_scale + hg_5_scale + hg_6_scale + hg_7_scale + hg_8_scale + hg_9_scale + hg_10_scale + hg_11_scale + a1_scale + depth1_scale + width1_scale + b1_scale, test)
```



```{r}
# get rid of model coefficients associated with obs_date
model_coefs = lm.train$coefficients[ !startsWith( names(lm.train$coefficients), "obs_date") ]
model_coefs[is.na(model_coefs)] = 0
```

```{r}
all( names(model_coefs) == colnames(Xtest) )
```

center:
```{r}
df.list[[day.num]] = data.frame(
  pred_rv = Xtest %*% model_coefs,
  pred_rv2 = Xtest %*% model_coefs + unique(pert.df$pert_val)[day.num],
  true_rv = test$rv_pert_center,
  lam_order = test$lam_order,
  obs_date = test$obs_date,
  snr = test$snr,
  wavelength = test$wavelength
  )
```

scale:
```{r}
df.list[[day.num]] = data.frame(
  pred_rv = Xtest %*% model_coefs,
  true_rv = test$rv_pert_scale,
  lam_order = test$lam_order,
  obs_date = test$obs_date,
  snr = test$snr,
  wavelength = test$wavelength
  )
```

```{r}
df.list
```

```{r}
pred.df = do.call(rbind, df.list)
pred.df
```

```{r}
mean( abs(pred.df$true_rv - pred.df$pred_rv) )
mean( abs(pred.df$true_rv - pred.df$pred_rv2) )
```

```{r}
df.list[[day.num]] %>%
  ggplot() +
  geom_point(mapping = aes(x = pred_rv2, y = true_rv, color = snr )) +
  geom_abline(slope=1,intercept=0,linetype = 2)
```

```{r}
df.list[[day.num]] %>%
  ggplot() +
  geom_point(mapping = aes(x = pred_rv, y = true_rv, color = snr )) +
  geom_abline(slope=1,intercept=0,linetype = 2)
```

```{r}
sqrt( mean( (df.list[[day.num]]$true_rv - df.list[[day.num]]$pred_rv2)^2 ) )
sqrt( mean( (df.list[[day.num]]$true_rv - df.list[[day.num]]$pred_rv)^2 ) )
```

center_rv model
  MSE for modified prediction (with day as perturbed value) and prediction with day coef as 0:
    12.44989
    12.57237

```{r}
rm(df.list, pred.df, test, train, lm.train, pert.df, amp, day.num, freq, Xtest, model_coefs)
```




